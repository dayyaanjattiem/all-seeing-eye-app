# -*- coding: utf-8 -*-
"""All Seeing Eye Iteration 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mnKxXUDrUCk1reGgW6opB4Feu9utHYRI
"""

!pip install pmdarima

# Import necessary libraries

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import yfinance as yf
import matplotlib.pyplot as plt
from datetime import timedelta
import yfinance as yf
import matplotlib.pyplot as plt
import pandas as pd
from pmdarima import auto_arima
from datetime import timedelta
import warnings

warnings.filterwarnings("ignore")

# Function to calculate RSI (Relative Strength Index)
def calculate_rsi(prices, window=14):
    delta = prices.diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=window).mean()
    avg_loss = loss.rolling(window=window).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi.iloc[-1] if not rsi.isna().all() else np.nan

import yfinance as yf
import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from pandas.plotting import register_matplotlib_converters
from datetime import timedelta
import warnings

# Suppress warnings and enable converters for datetime plotting
warnings.filterwarnings("ignore")
register_matplotlib_converters()

# Parameters
tickers = ["IMP.JO", "ANG.JO", "GFI.JO","VAL.JO","APN.JO","NPH.JO","CLS.JO","MRP.JO","BHG.JO","OMN.JO","PPC.JO","TBS.JO"]
forecast_days = 63  # ~3 months (trading days)

# Loop through each ticker
for ticker in tickers:
    print(f"\nProcessing stock: {ticker}")

    # Download stock data
    data = yf.download(ticker, period="5y")
    if data.empty or 'Close' not in data:
        print(f"‚ö†Ô∏è No data for {ticker}")
        continue

    data = data[['Close']].dropna()

    # Fit Holt-Winters Exponential Smoothing model (additive trend, no seasonality)
    model = ExponentialSmoothing(data['Close'], trend='add', seasonal=None, initialization_method="estimated")
    model_fit = model.fit()

    # Forecast next N trading days
    forecast = model_fit.forecast(forecast_days)

    # Create forecast date index
    last_date = data.index[-1]
    forecast_dates = pd.bdate_range(start=last_date + timedelta(days=1), periods=forecast_days)

    # Plot historical and forecasted prices
    plt.figure(figsize=(14, 6))
    plt.plot(data.index, data['Close'], label='Historical Price', color='blue')
    plt.plot(forecast_dates, forecast, label='Forecast', linestyle='--', color='red')
    plt.fill_between(forecast_dates, forecast * 0.9, forecast * 1.1, color='red', alpha=0.2, label='Confidence Interval (¬±10%)')
    plt.title(f"{ticker} - Holt-Winters Forecast (Next 3 Months)")
    plt.xlabel("Date")
    plt.ylabel("Price")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

#This is the second most refined version of the code, below this is the basic original which has overfitting as a flaw.

import yfinance as yf
import numpy as np
import matplotlib.pyplot as plt
from datetime import timedelta
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import warnings
warnings.filterwarnings("ignore")

# Parameters
tickers = ["IMP.JO"]
window_size = 30  # Number of timesteps (days) in each input sequence
forecast_days = 63  # ~3 months of trading days

for ticker in tickers:
    print(f"\nüîç Processing stock: {ticker}")

    stock = yf.Ticker(ticker)
    hist = stock.history(period="5y")

    if hist.empty:
        print(f"‚ö†Ô∏è Warning: No data for {ticker}")
        continue

    #Added the first fix over here
    closing = hist['Close'].values
    returns = np.diff(closing) / closing[:-1]
    returns = returns.reshape(-1, 1)

    scaler = StandardScaler()
    scaled_prices = scaler.fit_transform(closing_prices)

    # Create sequences
    X_all, y_all = [], []
    for i in range(window_size, len(scaled_prices) - forecast_days):
        X_all.append(scaled_prices[i - window_size:i])
        y_all.append(scaled_prices[i:i+forecast_days]) # Added the second fix here

    X = np.array(X_all)  # Shape: (samples, timesteps, 1)
    y = np.array(y_all).reshape(-1, forecast_days) #Part of the second fix

    # Train-test split
    split_ratio = 0.8
    split_index = int(len(X) * split_ratio)
    X_train, y_train = X[:split_index], y[:split_index]
    X_test, y_test = X[split_index:], y[split_index:]

    # Build LSTM model
    model = Sequential([
        LSTM(128, return_sequences=True, input_shape=(window_size, 1)),
        Dropout(0.3),
        LSTM(64),
        Dropout(0.2),
        Dense(forecast_days) # Change this Dense(1) with that, part of the second fix
    ])

    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
    history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), verbose=1)

    loss, mae = model.evaluate(X_test, y_test)
    print(f"‚úÖ Test MAE for {ticker}: {mae:.4f}")
    lookback = 100
    plt.figure(figsize=(14, 6))
    plt.plot(dates_index[-lookback:], closing_prices.flatten()[-lookback:], label=f"{ticker} Actual Prices")
    plt.plot(forecast_dates, forecast, label=f"{ticker} Forecasted Prices (6 months)", linestyle='--')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.title(f'LSTM Forecast for {ticker} (Next ~6 Months) - seasonality + spikes applied')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Save model
    model.save(f"{ticker}_lstm_forecast_model.h5")

    # Plot MAE and Loss
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['mae'], label='Train MAE')
    plt.plot(history.history['val_mae'], label='Validation MAE')
    plt.xlabel('Epochs'); plt.ylabel('Mean Absolute Error'); plt.title(f"{ticker} - MAE"); plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title(f"{ticker} - Loss"); plt.legend()
    plt.show()

import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import timedelta
import warnings
warnings.filterwarnings("ignore")

# Data preprocessing and ML
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import joblib

# Deep Learning
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import (LSTM, Dense, Dropout, BatchNormalization,
                                       Bidirectional, Input, Conv1D, MaxPooling1D,
                                       GlobalAveragePooling1D, GRU)
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.regularizers import l2
    from tensorflow.keras.optimizers import Adam
    TF_AVAILABLE = True
    print("‚úÖ TensorFlow loaded successfully")
except ImportError:
    TF_AVAILABLE = False
    print("‚ùå TensorFlow not available")

# Technical Analysis
try:
    import ta
    TA_AVAILABLE = True
    print("‚úÖ TA library loaded successfully")
except ImportError:
    TA_AVAILABLE = False
    print("‚ö†Ô∏è TA library not available - using basic indicators only")

# Statistical analysis
from scipy import stats

class EnhancedStockPredictor:
    """
    Enhanced LSTM Stock Prediction Model - COMPLETELY FIXED VERSION
    """

    def __init__(self, window_size=60, forecast_days=21, ensemble_size=3):
        self.window_size = window_size
        self.forecast_days = forecast_days
        self.ensemble_size = ensemble_size
        self.models = []
        self.scalers = []
        self.feature_names = []

    def add_technical_indicators(self, df):
        """
        Add technical indicators - COMPLETELY REWRITTEN TO FIX VOLATILITY ERROR
        """
        print("üîß Adding technical indicators...")
        features_df = df.copy()

        # Basic safety checks
        if len(df) < 60:  # Need enough data for indicators
            print("‚ö†Ô∏è Warning: Not enough data for technical indicators")
            basic_features = pd.DataFrame()
            basic_features['returns'] = df['Close'].pct_change()
            basic_features['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))
            return basic_features.fillna(0)

        try:
            # 1. BASIC PRICE FEATURES
            features_df['returns'] = df['Close'].pct_change()
            features_df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))

            # 2. MOVING AVERAGES - Calculate all periods first
            sma_periods = [5, 10, 20, 50]
            for period in sma_periods:
                features_df[f'sma_{period}'] = df['Close'].rolling(window=period, min_periods=1).mean()
                features_df[f'ema_{period}'] = df['Close'].ewm(span=period).mean()

            # Then calculate ratios
            for period in sma_periods:
                features_df[f'sma_ratio_{period}'] = (df['Close'] / features_df[f'sma_{period}'] - 1)
                features_df[f'ema_ratio_{period}'] = (df['Close'] / features_df[f'ema_{period}'] - 1)

            # 3. VOLATILITY FEATURES - FIXED APPROACH
            volatility_periods = [5, 10, 20]

            # STEP 1: Calculate ALL volatility columns first
            for period in volatility_periods:
                features_df[f'volatility_{period}'] = features_df['returns'].rolling(
                    window=period, min_periods=1
                ).std()

            # STEP 2: Now calculate volatility ratios (base on 20-day volatility)
            base_volatility = features_df['volatility_20'].replace(0, np.nan).fillna(1e-8)
            for period in [5, 10]:  # Don't include 20 (ratio would be 1)
                features_df[f'volatility_ratio_{period}'] = (
                    features_df[f'volatility_{period}'] / base_volatility
                )

            # 4. PRICE POSITION AND RANGE
            if all(col in df.columns for col in ['High', 'Low']):
                price_range = (df['High'] - df['Low']).replace(0, 1e-8)
                features_df['price_range'] = price_range / df['Close']
                features_df['price_position'] = (df['Close'] - df['Low']) / price_range
            else:
                features_df['price_range'] = 0
                features_df['price_position'] = 0.5

            # 5. VOLUME FEATURES
            if 'Volume' in df.columns and df['Volume'].sum() > 0:
                volume_sma = df['Volume'].rolling(window=20, min_periods=1).mean()
                features_df['volume_ratio'] = df['Volume'] / volume_sma.replace(0, 1)
                features_df['volume_change'] = df['Volume'].pct_change()
            else:
                features_df['volume_ratio'] = 1.0
                features_df['volume_change'] = 0.0

            # 6. MOMENTUM FEATURES
            momentum_periods = [5, 10, 20]
            for period in momentum_periods:
                features_df[f'momentum_{period}'] = (
                    df['Close'] / df['Close'].shift(period) - 1
                )
                features_df[f'roc_{period}'] = df['Close'].pct_change(periods=period)

            # 7. SUPPORT/RESISTANCE LEVELS
            if all(col in df.columns for col in ['High', 'Low']):
                features_df['high_20'] = df['High'].rolling(window=20, min_periods=1).max()
                features_df['low_20'] = df['Low'].rolling(window=20, min_periods=1).min()
                features_df['distance_to_high'] = (features_df['high_20'] - df['Close']) / df['Close']
                features_df['distance_to_low'] = (df['Close'] - features_df['low_20']) / df['Close']

            # 8. TECHNICAL INDICATORS (if TA available)
            if TA_AVAILABLE:
                try:
                    print("üìä Adding advanced technical indicators...")

                    # RSI
                    rsi = ta.momentum.RSIIndicator(df['Close'], window=14)
                    features_df['rsi'] = rsi.rsi()
                    features_df['rsi_normalized'] = (features_df['rsi'] - 50) / 50

                    # MACD
                    macd = ta.trend.MACD(df['Close'])
                    features_df['macd_line'] = macd.macd()
                    features_df['macd_signal'] = macd.macd_signal()
                    features_df['macd_histogram'] = macd.macd_diff()

                    # Bollinger Bands
                    bb = ta.volatility.BollingerBands(df['Close'])
                    bb_upper = bb.bollinger_hband()
                    bb_lower = bb.bollinger_lband()
                    bb_mid = bb.bollinger_mavg()

                    bb_width = (bb_upper - bb_lower).replace(0, 1e-8)
                    features_df['bb_position'] = (df['Close'] - bb_lower) / bb_width
                    features_df['bb_width'] = bb_width / df['Close']

                    # Stochastic Oscillator (only if OHLC available)
                    if all(col in df.columns for col in ['High', 'Low']):
                        stoch = ta.momentum.StochasticOscillator(df['High'], df['Low'], df['Close'])
                        features_df['stoch_k'] = stoch.stoch()
                        features_df['stoch_d'] = stoch.stoch_signal()

                        # ATR
                        atr = ta.volatility.AverageTrueRange(df['High'], df['Low'], df['Close'])
                        features_df['atr'] = atr.average_true_range()
                        features_df['atr_ratio'] = features_df['atr'] / df['Close']

                        # Williams %R
                        williams = ta.momentum.WilliamsRIndicator(df['High'], df['Low'], df['Close'])
                        features_df['williams_r'] = williams.williams_r()

                except Exception as e:
                    print(f"‚ö†Ô∏è Some technical indicators failed: {e}")

        except Exception as e:
            print(f"‚ö†Ô∏è Error in feature engineering: {e}")
            # Return basic features as fallback
            basic_features = pd.DataFrame()
            basic_features['returns'] = df['Close'].pct_change()
            basic_features['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))
            return basic_features.fillna(0)

        # CLEAN UP DATA
        print("üßπ Cleaning feature data...")

        # Replace infinite values
        features_df = features_df.replace([np.inf, -np.inf], np.nan)

        # Fill NaN values
        features_df = features_df.fillna(method='ffill').fillna(method='bfill').fillna(0)

        print(f"‚úÖ Created {len(features_df.columns)} total features")
        return features_df

    def prepare_features(self, df):
        """
        Prepare feature matrix from dataframe
        """
        print("üîß Preparing feature matrix...")

        # Get features with technical indicators
        features_df = self.add_technical_indicators(df)

        # Select relevant features (exclude raw OHLCV and intermediate calculations)
        exclude_cols = [
            'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits',
            'sma_5', 'sma_10', 'sma_20', 'sma_50',  # Keep ratios, not raw values
            'ema_5', 'ema_10', 'ema_20', 'ema_50',  # Keep ratios, not raw values
            'high_20', 'low_20'  # Keep distances, not raw values
        ]

        # Get feature columns
        available_cols = features_df.columns.tolist()
        feature_cols = [col for col in available_cols if col not in exclude_cols]

        # Ensure we have some features
        if len(feature_cols) == 0:
            print("‚ö†Ô∏è No advanced features, using basic ones")
            feature_cols = ['returns', 'log_returns']

        # Extract feature matrix
        feature_matrix = features_df[feature_cols].values

        # Final cleanup
        feature_matrix = np.nan_to_num(feature_matrix, nan=0.0, posinf=0.0, neginf=0.0)

        self.feature_names = feature_cols
        print(f"üìä Using {len(feature_cols)} features")

        return feature_matrix

    def robust_scaling(self, data, fit_data=None):
        """
        Apply robust scaling
        """
        if fit_data is not None:
            # Remove extreme outliers
            z_scores = np.abs(stats.zscore(fit_data, axis=0, nan_policy='omit'))
            outlier_mask = np.any(z_scores > 4, axis=1)  # More conservative threshold
            clean_fit_data = fit_data[~outlier_mask]

            scaler = RobustScaler()
            scaler.fit(clean_fit_data)
            return scaler.transform(data), scaler
        else:
            return data

    def create_sequences(self, data, target_col_idx=0):
        """
        Create training sequences
        """
        X, y = [], []

        for i in range(self.window_size, len(data) - self.forecast_days + 1):
            X.append(data[i - self.window_size:i])
            y.append(data[i:i + self.forecast_days, target_col_idx])

        return np.array(X), np.array(y)

    def create_model(self, input_shape, model_type='bidirectional'):
        """
        Create LSTM model
        """
        if model_type == 'bidirectional':
            model = Sequential([
                Bidirectional(LSTM(128, return_sequences=True, dropout=0.1),
                            input_shape=input_shape),
                BatchNormalization(),

                Bidirectional(LSTM(64, return_sequences=True, dropout=0.1)),
                BatchNormalization(),

                Bidirectional(LSTM(32, dropout=0.1)),
                BatchNormalization(),

                Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
                Dropout(0.2),
                Dense(32, activation='relu'),
                Dropout(0.1),
                Dense(self.forecast_days)
            ])

        elif model_type == 'standard':
            model = Sequential([
                LSTM(128, return_sequences=True, input_shape=input_shape, dropout=0.1),
                BatchNormalization(),
                LSTM(64, return_sequences=True, dropout=0.1),
                BatchNormalization(),
                LSTM(32, dropout=0.1),
                BatchNormalization(),
                Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
                Dropout(0.2),
                Dense(self.forecast_days)
            ])

        elif model_type == 'gru':
            model = Sequential([
                GRU(128, return_sequences=True, input_shape=input_shape, dropout=0.1),
                BatchNormalization(),
                GRU(64, return_sequences=True, dropout=0.1),
                BatchNormalization(),
                GRU(32, dropout=0.1),
                BatchNormalization(),
                Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
                Dropout(0.2),
                Dense(self.forecast_days)
            ])

        return model

    def create_callbacks(self):
        """
        Create training callbacks
        """
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=25,
            restore_best_weights=True,
            min_delta=1e-6
        )

        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.7,
            patience=12,
            min_lr=1e-5,
            verbose=1
        )

        return [early_stopping, reduce_lr]

    def calculate_metrics(self, y_true, y_pred):
        """
        Calculate evaluation metrics
        """
        if y_true.ndim > 1:
            y_true = y_true.flatten()
        if y_pred.ndim > 1:
            y_pred = y_pred.flatten()

        # Basic metrics
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))

        # Directional accuracy
        if len(y_true) > 1:
            true_direction = np.sign(y_true[1:] - y_true[:-1])
            pred_direction = np.sign(y_pred[1:] - y_pred[:-1])
            directional_accuracy = np.mean(true_direction == pred_direction)
        else:
            directional_accuracy = 0.5

        # MAPE
        mask = np.abs(y_true) > 1e-6
        if np.any(mask):
            mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        else:
            mape = 0

        return {
            'MAE': mae,
            'RMSE': rmse,
            'MAPE': mape,
            'Directional_Accuracy': directional_accuracy
        }

    def train_model(self, X_train, y_train, X_val, y_val, model_type='bidirectional'):
        """
        Train a single model
        """
        print(f"üèóÔ∏è Training {model_type} model...")

        model = self.create_model(X_train.shape[1:], model_type)

        optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
        model.compile(optimizer=optimizer, loss='huber', metrics=['mae'])

        callbacks = self.create_callbacks()

        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=100,
            batch_size=32,
            callbacks=callbacks,
            verbose=1
        )

        return model, history

    def train_ensemble(self, X_train, y_train, X_val, y_val):
        """
        Train ensemble of models
        """
        model_types = ['bidirectional', 'standard', 'gru'][:self.ensemble_size]

        for model_type in model_types:
            try:
                model, history = self.train_model(X_train, y_train, X_val, y_val, model_type)
                self.models.append(model)
                print(f"‚úÖ {model_type} model trained successfully")
            except Exception as e:
                print(f"‚ùå Failed to train {model_type} model: {e}")

    def ensemble_predict(self, X):
        """
        Make ensemble predictions
        """
        if not self.models:
            print("‚ùå No trained models available")
            return np.zeros((X.shape[0], self.forecast_days))

        predictions = []
        for model in self.models:
            pred = model.predict(X, verbose=0)
            predictions.append(pred)

        return np.mean(predictions, axis=0)

    def train(self, df, target_column='Close'):
        """
        Main training function
        """
        print("\nüéØ Training Enhanced LSTM Ensemble")
        print("=" * 50)

        try:
            # Prepare features
            feature_matrix = self.prepare_features(df)

            # Prepare target
            prices = df[target_column].values
            log_returns = np.log(prices[1:] / prices[:-1]).reshape(-1, 1)

            # Align features with returns
            feature_matrix = feature_matrix[1:]

            # Combine all data
            all_data = np.column_stack([log_returns, feature_matrix])

            print(f"üìä Data shape: {all_data.shape}")

            # Check data sufficiency
            min_required = self.window_size + self.forecast_days + 50
            if len(all_data) < min_required:
                print(f"‚ùå Need at least {min_required} data points, got {len(all_data)}")
                return None

            # Split data temporally
            train_size = int(len(all_data) * 0.7)
            val_size = int(len(all_data) * 0.15)

            train_data = all_data[:train_size]
            val_data = all_data[train_size:train_size + val_size]
            test_data = all_data[train_size + val_size:]

            # Scale data
            print("üîÑ Scaling data...")
            scaled_train, scaler = self.robust_scaling(train_data, train_data)
            scaled_val = scaler.transform(val_data)
            scaled_test = scaler.transform(test_data)
            self.scalers.append(scaler)

            # Create sequences
            print("üî¢ Creating sequences...")
            X_train, y_train = self.create_sequences(scaled_train)
            X_val, y_val = self.create_sequences(scaled_val)
            X_test, y_test = self.create_sequences(scaled_test)

            print(f"üìä Training: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}")

            if len(X_train) == 0:
                print("‚ùå No training sequences created")
                return None

            # Train models
            if TF_AVAILABLE:
                self.train_ensemble(X_train, y_train, X_val, y_val)

                if self.models:
                    # Evaluate
                    print("\nüìà Evaluating ensemble...")
                    y_pred = self.ensemble_predict(X_test)
                    metrics = self.calculate_metrics(y_test, y_pred)

                    print("\nüèÜ Performance:")
                    for metric, value in metrics.items():
                        if 'Accuracy' in metric:
                            print(f"  {metric}: {value:.2%}")
                        else:
                            print(f"  {metric}: {value:.4f}")

                    return metrics
                else:
                    print("‚ùå No models were trained successfully")
                    return None
            else:
                print("‚ùå TensorFlow not available")
                return None

        except Exception as e:
            print(f"‚ùå Training failed: {e}")
            import traceback
            traceback.print_exc()
            return None

    def predict_future(self, df, days_ahead=None):
        """
        Predict future prices
        """
        if not self.models or not self.scalers:
            print("‚ùå Model not trained")
            return np.array([])

        if days_ahead is None:
            days_ahead = self.forecast_days

        try:
            # Prepare data
            feature_matrix = self.prepare_features(df)
            prices = df['Close'].values
            log_returns = np.log(prices[1:] / prices[:-1]).reshape(-1, 1)
            feature_matrix = feature_matrix[1:]

            all_data = np.column_stack([log_returns, feature_matrix])
            scaled_data = self.scalers[0].transform(all_data)

            # Get prediction input
            last_sequence = scaled_data[-self.window_size:].reshape(1, self.window_size, -1)

            # Predict
            scaled_forecast = self.ensemble_predict(last_sequence)[0]

            # Convert back to returns
            dummy_features = np.zeros((len(scaled_forecast), scaled_data.shape[1]))
            dummy_features[:, 0] = scaled_forecast
            forecast_returns = self.scalers[0].inverse_transform(dummy_features)[:, 0]

            # Convert to prices
            last_price = prices[-1]
            forecast_prices = [last_price]

            for ret in forecast_returns[:days_ahead]:
                next_price = forecast_prices[-1] * np.exp(ret)
                forecast_prices.append(next_price)

            return np.array(forecast_prices[1:])

        except Exception as e:
            print(f"‚ùå Prediction failed: {e}")
            return np.array([])


def main():
    """
    Main function - COMPLETELY FIXED VERSION
    """
    # Configuration
    TICKER = "IMP.JO"
    WINDOW_SIZE = 60
    FORECAST_DAYS = 21
    ENSEMBLE_SIZE = 3

    print("üöÄ Enhanced LSTM Stock Predictor - FIXED VERSION")
    print("=" * 55)

    try:
        # Download data
        print(f"üì• Downloading {TICKER} data...")
        stock = yf.Ticker(TICKER)
        hist = stock.history(period="5y")

        if hist.empty:
            print(f"‚ùå No data for {TICKER}")
            return

        print(f"üìä Downloaded {len(hist)} days of data")

        # Initialize predictor
        predictor = EnhancedStockPredictor(
            window_size=WINDOW_SIZE,
            forecast_days=FORECAST_DAYS,
            ensemble_size=ENSEMBLE_SIZE
        )

        # Train model
        metrics = predictor.train(hist)

        if metrics:
            # Generate predictions
            print("\nüîÆ Generating forecast...")
            future_prices = predictor.predict_future(hist)

            if len(future_prices) > 0:
                # Create forecast dates
                last_date = hist.index[-1]
                forecast_dates = pd.date_range(
                    start=last_date + pd.Timedelta(days=1),
                    periods=FORECAST_DAYS,
                    freq='B'
                )

                # Plot results
                plt.figure(figsize=(15, 8))

                lookback = 100
                hist_data = hist['Close'].values[-lookback:]
                hist_dates = hist.index[-lookback:]

                plt.plot(hist_dates, hist_data,
                        label=f'{TICKER} Historical', linewidth=2, color='blue')

                plt.plot(forecast_dates, future_prices,
                        label=f'{TICKER} Forecast ({FORECAST_DAYS} days)',
                        linewidth=2, linestyle='--', color='red')

                # Add uncertainty band
                std_dev = np.std(future_prices) * 0.3
                upper_band = future_prices + std_dev
                lower_band = future_prices - std_dev

                plt.fill_between(forecast_dates, lower_band, upper_band,
                               alpha=0.2, color='red', label='Uncertainty Band')

                plt.title(f'LSTM Ensemble Forecast - {TICKER} (FIXED VERSION)', fontsize=14)
                plt.xlabel('Date')
                plt.ylabel('Price ($)')
                plt.legend()
                plt.grid(True, alpha=0.3)
                plt.xticks(rotation=45)
                plt.tight_layout()
                plt.show()

                # Summary
                current_price = hist['Close'].values[-1]
                predicted_price = future_prices[-1]
                expected_return = (predicted_price / current_price - 1) * 100

                print(f"\nüìä FORECAST SUMMARY:")
                print(f"Current Price: ${current_price:.2f}")
                print(f"Predicted Price ({FORECAST_DAYS} days): ${predicted_price:.2f}")
                print(f"Expected Return: {expected_return:+.2f}%")
                print(f"Price Range: ${future_prices.min():.2f} - ${future_prices.max():.2f}")

                print(f"\nüéØ MODEL PERFORMANCE:")
                print(f"MAE: {metrics['MAE']:.4f}")
                print(f"RMSE: {metrics['RMSE']:.4f}")
                print(f"Directional Accuracy: {metrics['Directional_Accuracy']:.1%}")

                if metrics['Directional_Accuracy'] > 0.55:
                    print("‚úÖ Good directional prediction capability!")
                else:
                    print("‚ö†Ô∏è Moderate directional prediction capability")

            else:
                print("‚ùå Failed to generate predictions")
        else:
            print("‚ùå Model training failed")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

index_ticker = "^J141.JO"
all_tickers = tickers + [index_ticker]

# Fetch historical data
data = {}
for ticker in all_tickers:
    stock = yf.Ticker(ticker)
    hist = stock.history(period="5y")
    if not hist.empty:
        data[ticker] = hist['Close']

# Combine and normalize
df = pd.DataFrame(data).dropna()
normalized = df / df.iloc[0] * 100  # Normalize to 100-base

# Plot
plt.figure(figsize=(14, 7))
for ticker in tickers:
    plt.plot(normalized.index, normalized[ticker], label=ticker)
plt.plot(normalized.index, normalized[index_ticker], label='Sharia All Share Index', linestyle='--', linewidth=2.5, color='black')

plt.title("Normalized Stock Prices vs Share all share index (Last 5 Years)")
plt.xlabel("Date")
plt.ylabel("Normalized Price (Base = 100)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Calculate correlation with Sharia Index
correlations = {}

for ticker in tickers:
    corr = df[ticker].corr(df[index_ticker])
    correlations[ticker] = corr
    print(f"üìà Correlation between {ticker} and Sharia Index: {corr:.4f}")


correlation_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Correlation with Sharia Index'])
print(correlation_df.sort_values(by='Correlation with Sharia Index', ascending=False))

import statsmodels.api as sm

# Compute daily log returns
log_returns = np.log(df / df.shift(1)).dropna()

# Dictionary to store Betas
betas = {}

for ticker in tickers:
    X = log_returns[index_ticker]  # Market returns (Sharia Index)
    y = log_returns[ticker]        # Stock returns

    X = sm.add_constant(X)  # Add constant/intercept
    model = sm.OLS(y, X).fit()
    beta = model.params[1]  # Extract beta coefficient
    betas[ticker] = beta

# Convert to DataFrame
beta_df = pd.DataFrame.from_dict(betas, orient='index', columns=['Beta vs Sharia Index'])
print(beta_df)

# Optional: Plot as bar chart
plt.figure(figsize=(10, 5))
beta_df['Beta vs Sharia Index'].plot(kind='bar', color='skyblue')
plt.title('Stock Beta vs Sharia Top 40 Index calculated on daily returns')
plt.ylabel('Beta Value')
plt.xlabel('Stock Ticker')
plt.axhline(1.0, color='red', linestyle='--', linewidth=1)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# improved_probabilistic_lstm_fixed.py
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import timedelta
import joblib
import warnings
warnings.filterwarnings("ignore")

import yfinance as yf
from scipy import stats

from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# TensorFlow / Keras
try:
    import tensorflow as tf
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, BatchNormalization, Bidirectional
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
    TF_AVAILABLE = True
    print("‚úÖ TensorFlow available")
except Exception as e:
    TF_AVAILABLE = False
    print("‚ùå TensorFlow not available:", e)

# Optional: advanced TA ‚Äî if present use it, else fallback to built-in features
try:
    import ta
    TA_AVAILABLE = True
    print("‚úÖ ta (technical analysis) library available")
except Exception:
    TA_AVAILABLE = False
    print("‚ö†Ô∏è ta library not available ‚Äî using internal indicators only")

# *** EXTERNAL DATA DOWNLOADER ***
try:
    import pandas_datareader.data as web
    PDR_AVAILABLE = True
    print("‚úÖ pandas_datareader available for FRED/macro data")
except ImportError:
    PDR_AVAILABLE = False
    print("‚ö†Ô∏è pandas_datareader not available. Install it (pip install pandas-datareader).")

# -------------------------
# NEW: Data Cleaning Helpers
# -------------------------
def clean_ticker_data(df):
    """
    Flattens yfinance MultiIndex columns (if any) and standardizes column names
    to Title Case (Open, High, Low, Close, Volume).
    """
    if df is None or df.empty:
        return df

    df = df.copy()

    # 1. Flatten MultiIndex columns (e.g. ('Close', 'APN.JO') -> 'Close')
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(0)

    # 2. Ensure Title Case keys
    # Map common variations to standard names
    col_map = {
        'date': 'Date',
        'open': 'Open',
        'high': 'High',
        'low': 'Low',
        'close': 'Close',
        'volume': 'Volume',
        'adj close': 'Adj Close'
    }
    df.rename(columns=lambda x: col_map.get(x.lower(), x), inplace=True)

    # 3. Remove Timezone from index
    if df.index.tz is not None:
        df.index = df.index.tz_localize(None)

    return df

def clean_series_data(df, col_name_target):
    """
    Helper to force any macro DataFrame into a clean Single-Index, Single-Column format.
    """
    if df is None or df.empty:
        return pd.DataFrame()

    df = df.copy()

    # Flatten Column MultiIndex
    if isinstance(df.columns, pd.MultiIndex):
        try:
            if 'Close' in df.columns.get_level_values(0):
                df = df.xs('Close', axis=1, level=0, drop_level=True)
        except:
            pass
        # Fallback: flatten names
        if isinstance(df.columns, pd.MultiIndex):
             df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]

    # Ensure 1 column
    if df.shape[1] > 1:
        if 'Close' in df.columns:
            df = df[['Close']]
        else:
            df = df.iloc[:, :1]

    df.columns = [col_name_target]

    # Clean Index
    if df.index.tz is not None:
        df.index = df.index.tz_localize(None)

    # Remove extra index levels
    while df.index.nlevels > 1:
        df.index = df.index.droplevel(1)

    return df

def fetch_macro_data(start_date, end_date):
    """Downloads and merges macro data safely using pd.concat."""
    end_date_fetch = end_date + timedelta(days=1)
    data_frames = []

    # 1. Benchmark Index (S&P 500)
    try:
        spx = yf.download('^GSPC', start=start_date, end=end_date_fetch, progress=False)
        spx_clean = clean_series_data(spx, 'SPX_Close')
        if not spx_clean.empty:
            data_frames.append(spx_clean)
    except Exception as e:
        print(f"‚ö†Ô∏è SPX download failed: {e}")

    # 2. Volatility Index (VIX)
    try:
        vix = yf.download('^VIX', start=start_date, end=end_date_fetch, progress=False)
        vix_clean = clean_series_data(vix, 'VIX_Index')
        if not vix_clean.empty:
            data_frames.append(vix_clean)
    except Exception as e:
        print(f"‚ö†Ô∏è VIX download failed: {e}")

    # 3. FRED Data
    if PDR_AVAILABLE:
        try:
            unemp = web.DataReader('UNRATE', 'fred', start=start_date, end=end_date_fetch)
            unemp_clean = clean_series_data(unemp, 'Unemployment_Rate')
            if not unemp_clean.empty:
                data_frames.append(unemp_clean)

            cpi = web.DataReader('CPIAUCSL', 'fred', start=start_date, end=end_date_fetch)
            cpi_clean = clean_series_data(cpi, 'CPI')
            if not cpi_clean.empty:
                data_frames.append(cpi_clean)
        except Exception as e:
            print(f"‚ö†Ô∏è FRED data download failed: {e}")

    if not data_frames:
        return pd.DataFrame()

    macro_df = pd.concat(data_frames, axis=1)
    macro_df = macro_df.ffill()
    return macro_df

def ensure_minimum_length(df, min_len=400):
    if len(df) < min_len:
        print(f"‚ö†Ô∏è Note: Only {len(df)} rows available; recommended >= {min_len} for reliable 3-month forecasts.")
    return df

# -------------------------
# Predictor class
# -------------------------
class ProbabilisticEnsemblePredictor:
    def __init__(self,
                 window_size=60,
                 forecast_days=63,
                 ensemble_size=5,
                 mc_dropout_runs=200,
                 model_dir="saved_models",
                 max_abs_value=1e8):
        self.window_size = window_size
        self.forecast_days = forecast_days
        self.ensemble_size = ensemble_size
        self.mc_runs = mc_dropout_runs
        self.models = []
        self.scalers = []
        self.feature_names = None
        self.model_dir = model_dir
        self.max_abs_value = max_abs_value
        os.makedirs(self.model_dir, exist_ok=True)

    # -------------------------
    # Feature engineering
    # -------------------------
    def add_features(self, df):
        # Clean input data structure first
        df = clean_ticker_data(df)

        # 1. --- External/Macro Features ---
        start_date = df.index.min()
        end_date = df.index.max()
        macro_df = fetch_macro_data(start_date, end_date)

        if not macro_df.empty:
            df = df.join(macro_df, how='left')

        # Calculate features from macro data
        if 'SPX_Close' in df.columns:
            df['SPX_log_returns'] = np.log(df['SPX_Close'] / df['SPX_Close'].shift(1)).replace([np.inf, -np.inf], 0).fillna(0)
            if 'Close' in df.columns:
                df['relative_strength'] = (df['Close'] / df['SPX_Close']).replace([np.inf, -np.inf], 1).fillna(1)

        if 'VIX_Index' in df.columns:
            df['VIX_log'] = np.log(df['VIX_Index']).replace([np.inf, -np.inf], 0).fillna(0)

        # 2. --- Micro/Time-Based Features ---
        df['day_of_week'] = df.index.dayofweek
        df['day_of_month'] = df.index.day
        df['is_month_start'] = df.index.is_month_start.astype(int)
        df['is_month_end'] = df.index.is_month_end.astype(int)

        # 3. --- Existing Core Features ---
        # Ensure 'Close' exists before creating 'close'
        if 'Close' not in df.columns:
            raise KeyError(f"Column 'Close' not found. Available columns: {df.columns.tolist()}")

        df['close'] = df['Close']
        df['returns'] = df['close'].pct_change().fillna(0)
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1)).replace([np.inf, -np.inf], 0).fillna(0)

        periods = [5, 10, 20, 50, 100, 200]
        for p in periods:
            df[f'sma_{p}'] = df['close'].rolling(p, min_periods=1).mean()
            df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()
            df[f'sma_ratio_{p}'] = (df['close'] / df[f'sma_{p}'] - 1).replace([np.inf, -np.inf], 0).fillna(0)
            df[f'ema_ratio_{p}'] = (df['close'] / df[f'ema_{p}'] - 1).replace([np.inf, -np.inf], 0).fillna(0)

        vol_periods = [5, 10, 20, 60]
        for p in vol_periods:
            df[f'vol_{p}'] = df['log_returns'].rolling(p, min_periods=1).std().fillna(0)
        base_vol = df['vol_20'].replace(0, 1e-8)
        df['vol_ratio_5_20'] = (df['vol_5'] / base_vol).replace([np.inf, -np.inf], 0).fillna(0)
        df['vol_ratio_10_20'] = (df['vol_10'] / base_vol).replace([np.inf, -np.inf], 0).fillna(0)

        if all(c in df.columns for c in ['High', 'Low']):
            price_range = (df['High'] - df['Low']).replace(0, 1e-8)
            df['price_range_pct'] = (price_range / df['close']).replace([np.inf, -np.inf], 0).fillna(0)
            df['price_pos'] = ((df['close'] - df['Low']) / price_range).replace([np.inf, -np.inf], 0).fillna(0.5)
        else:
            df['price_range_pct'] = 0
            df['price_pos'] = 0.5

        if 'Volume' in df.columns:
            df['vol_sma_20'] = df['Volume'].rolling(20, min_periods=1).mean().replace(0, 1)
            df['vol_ratio'] = (df['Volume'] / df['vol_sma_20']).replace([np.inf, -np.inf], 1).fillna(1)
            df['vol_change'] = df['Volume'].pct_change().replace([np.inf, -np.inf], 0).fillna(0)
        else:
            df['vol_ratio'] = 1.0
            df['vol_change'] = 0.0

        momentum_periods = [5, 10, 20]
        for p in momentum_periods:
            df[f'mom_{p}'] = (df['close'] / df['close'].shift(p) - 1).replace([np.inf, -np.inf], 0).fillna(0)
            df[f'roc_{p}'] = df['close'].pct_change(periods=p).replace([np.inf, -np.inf], 0).fillna(0)

        if TA_AVAILABLE:
            try:
                rsi = ta.momentum.RSIIndicator(df['close'], window=14).rsi().fillna(50)
                df['rsi'] = rsi
                macd = ta.trend.MACD(df['close'])
                df['macd'] = macd.macd().fillna(0)
                bb = ta.volatility.BollingerBands(df['close'])
                bb_width = (bb.bollinger_hband() - bb.bollinger_lband()).replace(0, 1e-8)
                df['bb_width_pct'] = (bb_width / df['close']).replace([np.inf, -np.inf], 0).fillna(0)
            except Exception:
                pass

        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
        df = df.clip(-self.max_abs_value, self.max_abs_value)
        return df

    def prepare_matrices(self, raw_df, target_col='Close'):
        df = raw_df.copy()
        df = self.add_features(df)

        prices = df['close'].values
        prices_safe = np.where(prices <= 0, np.nan, prices)
        log_returns = np.log(prices_safe[1:] / prices_safe[:-1]).reshape(-1, 1)

        feature_df = df.iloc[1:].copy()

        exclude = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close',
                   'SPX_Close', 'VIX_Index', 'CPI', 'Unemployment_Rate']

        feature_cols = [c for c in feature_df.columns if c not in exclude]
        self.feature_names = feature_cols

        X = feature_df[feature_cols].values
        y = log_returns

        X = np.where(np.isfinite(X), X, np.nan)
        y = np.where(np.isfinite(y), y, np.nan)

        invalid_mask = np.logical_or(np.isnan(X).any(axis=1), np.isnan(y).any(axis=1))
        invalid_mask = np.logical_or(invalid_mask, (np.abs(X) > self.max_abs_value).any(axis=1))
        invalid_mask = np.logical_or(invalid_mask, (np.abs(y) > self.max_abs_value).any(axis=1))

        if np.sum(invalid_mask) > 0:
            print(f"‚ö†Ô∏è prepare_matrices: Removing {np.sum(invalid_mask)} invalid rows.")

        valid_mask = ~invalid_mask
        X_clean = X[valid_mask]
        y_clean = y[valid_mask]
        df_clean = feature_df.iloc[valid_mask].copy()
        df_clean['close'] = df['close'].iloc[1:][valid_mask]

        return X_clean, y_clean, df_clean

    def scale_data(self, X_train, X_val, X_test):
        scaler = RobustScaler()
        scaler.fit(X_train)
        X_train_s = scaler.transform(X_train)
        X_val_s = scaler.transform(X_val)
        X_test_s = scaler.transform(X_test)
        return X_train_s, X_val_s, X_test_s, scaler

    def create_sequences(self, X, y):
        Xs, ys = [], []
        for i in range(self.window_size, len(X) - self.forecast_days + 1):
            Xs.append(X[i - self.window_size:i])
            ys.append(y[i:i + self.forecast_days].reshape(-1))
        return np.array(Xs), np.array(ys)

    def build_model(self, n_features):
        inp = Input(shape=(self.window_size, n_features))
        x = Bidirectional(LSTM(128, return_sequences=True))(inp)
        x = BatchNormalization()(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x)
        x = BatchNormalization()(x)
        x = Bidirectional(LSTM(32))(x)
        x = BatchNormalization()(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.25)(x)
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.15)(x)
        out = Dense(self.forecast_days, name='out')(x)
        model = Model(inputs=inp, outputs=out)
        optimizer = Adam(learning_rate=1e-3)
        model.compile(optimizer=optimizer, loss='huber', metrics=['mae'])
        return model

    def train(self, raw_df, min_history=800):
        if not TF_AVAILABLE:
            print("‚ùå TensorFlow not available.")
            return None

        # Ensure we have clean data before checking length
        df = clean_ticker_data(raw_df)
        df = ensure_minimum_length(df, min_history)

        X_all, y_all, processed_df = self.prepare_matrices(df)
        n_samples = len(X_all)
        print(f"üìä Prepared {n_samples} aligned samples.")

        if n_samples < (self.window_size + self.forecast_days + 50):
            print("‚ùå Not enough data to train.")
            return None

        train_end = int(n_samples * 0.7)
        val_end = int(n_samples * 0.85)

        X_train = X_all[:train_end]
        y_train = y_all[:train_end].reshape(-1, 1)
        X_val = X_all[train_end:val_end]
        y_val = y_all[train_end:val_end].reshape(-1, 1)
        X_test = X_all[val_end:]
        y_test = y_all[val_end:].reshape(-1, 1)

        X_train_s, X_val_s, X_test_s, scaler = self.scale_data(X_train, X_val, X_test)
        self.scalers = [scaler]

        X_train_seq, y_train_seq = self.create_sequences(X_train_s, y_train)
        X_val_seq, y_val_seq = self.create_sequences(X_val_s, y_val)
        X_test_seq, y_test_seq = self.create_sequences(X_test_s, y_test)

        print(f"üî¢ Sequences: train={X_train_seq.shape}, val={X_val_seq.shape}, test={X_test_seq.shape}")

        if X_train_seq.shape[0] == 0:
            print("‚ùå No sequences created.")
            return None

        for i in range(self.ensemble_size):
            print(f"üèóÔ∏è Training model {i+1}/{self.ensemble_size}")
            model = self.build_model(n_features=X_train_seq.shape[2])
            callbacks = [
                EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, min_delta=1e-6),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-6, verbose=1)
            ]
            ckpt_path = os.path.join(self.model_dir, f"model_ensemble_{i}.h5")
            callbacks.append(ModelCheckpoint(ckpt_path, save_best_only=True, monitor='val_loss', verbose=0))

            model.fit(
                X_train_seq, y_train_seq,
                validation_data=(X_val_seq, y_val_seq),
                epochs=200,
                batch_size=32,
                callbacks=callbacks,
                verbose=1
            )
            if os.path.exists(ckpt_path):
                model.load_weights(ckpt_path)

            self.models.append(model)
            print(f"‚úÖ Model {i+1} saved.")

        y_pred = self.ensemble_predict(X_test_seq, mc_dropout=False).reshape(-1)
        y_true = y_test_seq.reshape(-1)
        metrics = self.calculate_metrics(y_true, y_pred)
        print("üîç Evaluation on test set:", metrics)
        return metrics

    def ensemble_predict(self, X_seq, mc_dropout=True, mc_runs=None):
        if mc_runs is None:
            mc_runs = self.mc_runs

        preds = []
        if mc_dropout:
            for model in self.models:
                model_preds = []
                runs_per_model = max(1, mc_runs // max(1, len(self.models)))
                for _ in range(runs_per_model):
                    prediction = model(X_seq, training=True).numpy()
                    if prediction.ndim == 2:
                        prediction = prediction[np.newaxis, :, :]
                    model_preds.append(prediction)
                model_preds = np.vstack(model_preds)
                preds.append(model_preds)
            preds = np.vstack(preds)
            return preds
        else:
            model_outs = [m.predict(X_seq, verbose=0) for m in self.models]
            mean_out = np.mean(model_outs, axis=0)
            return mean_out

    def predict_future(self, raw_df, days_ahead=None):
        if days_ahead is None:
            days_ahead = self.forecast_days
        if len(self.models) == 0:
            print("‚ùå Models not trained")
            return None

        # Clean data before processing
        df = clean_ticker_data(raw_df)

        X_all, y_all, processed_df = self.prepare_matrices(df)
        scaler = self.scalers[0]
        X_all_s = scaler.transform(X_all)

        if len(X_all_s) < self.window_size:
            print("‚ùå Not enough samples.")
            return None

        last_seq = X_all_s[-self.window_size:].reshape(1, self.window_size, -1)
        samples = self.ensemble_predict(last_seq, mc_dropout=True, mc_runs=self.mc_runs)

        if samples.ndim == 3:
            if samples.shape[1] == 1:
                samples = samples.squeeze(axis=1)
            elif samples.shape[1] > 1:
                samples = samples[:, -1, :]

        if samples.ndim == 1:
            samples = samples.reshape(1, -1)

        if 'close' in processed_df.columns:
            last_price = processed_df['close'].values[-1]
        else:
            last_price = df['Close'].values[-1]

        price_paths = []
        for i in range(samples.shape[0]):
            returns_seq = samples[i]
            prices = [last_price]
            for r in returns_seq[:days_ahead]:
                next_price = prices[-1] * np.exp(r)
                prices.append(next_price)
            price_paths.append(prices[1:])
        price_paths = np.array(price_paths)

        results = {
            'mean': np.mean(price_paths, axis=0),
            'median': np.median(price_paths, axis=0),
            'p5': np.percentile(price_paths, 5, axis=0),
            'p25': np.percentile(price_paths, 25, axis=0),
            'p75': np.percentile(price_paths, 75, axis=0),
            'p95': np.percentile(price_paths, 95, axis=0),
            'raw_paths': price_paths
        }
        return results

    def calculate_metrics(self, y_true, y_pred):
        y_true = np.array(y_true).reshape(-1)
        y_pred = np.array(y_pred).reshape(-1)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        if len(y_true) > 1:
            td = np.sign(y_true[1:] - y_true[:-1])
            pd = np.sign(y_pred[1:] - y_pred[:-1])
            directional = np.mean(td == pd)
        else:
            directional = 0.5
        mask = np.abs(y_true) > 1e-6
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if np.any(mask) else np.nan
        return {'MAE': mae, 'RMSE': rmse, 'MAPE%': mape, 'Directional_Accuracy': directional}

    def explain_results(self, metrics, forecast_results, raw_df):
        print("\n=== MODEL SUMMARY ===")
        if metrics:
            print(f"MAE: {metrics['MAE']:.6f} | RMSE: {metrics['RMSE']:.6f}")
            print(f"Directional Acc: {metrics['Directional_Accuracy']:.2%}")

        if forecast_results is None:
            return

        mean = forecast_results['mean']
        p5, p95 = forecast_results['p5'], forecast_results['p95']

        # Ensure we get the last price correctly from cleaned data if necessary
        last_price = raw_df['Close'].iloc[-1] if not isinstance(raw_df['Close'], pd.DataFrame) else raw_df['Close'].iloc[-1].values[0]

        final_mean = mean[-1]

        print("\n=== FORECAST ===")
        print(f"Last Price: {last_price:.2f}")
        print(f"Pred Mean: {final_mean:.2f}")
        print(f"90% Interval: [{p5[-1]:.2f}, {p95[-1]:.2f}]")

# -------------------------
# Main
# -------------------------
def main():
    TICKER = "APN.JO"
    WINDOW_SIZE = 60
    FORECAST_DAYS = 63
    ENSEMBLE_SIZE = 5
    MC_RUNS = 200

    print("üöÄ Improved Probabilistic LSTM Ensemble")
    print("="*60)

    try:
        ticker = yf.Ticker(TICKER)
        hist = ticker.history(period="10y")
        if hist.empty:
            print("‚ùå No data for", TICKER)
            return

        # Clean immediately after download to prevent column issues
        hist = clean_ticker_data(hist)

    except Exception as e:
        print("‚ùå Download failed:", e)
        return

    print(f"üì• Downloaded {len(hist)} rows for {TICKER}")

    predictor = ProbabilisticEnsemblePredictor(
        window_size=WINDOW_SIZE,
        forecast_days=FORECAST_DAYS,
        ensemble_size=ENSEMBLE_SIZE,
        mc_dropout_runs=MC_RUNS,
        model_dir="saved_models"
    )

    metrics = predictor.train(hist)
    if metrics is None:
        return

    forecast_results = predictor.predict_future(hist, days_ahead=FORECAST_DAYS)
    predictor.explain_results(metrics, forecast_results, hist)

    try:
        last_date = hist.index[-1]
        business_days = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=FORECAST_DAYS)
        plt.figure(figsize=(14,7))
        lookback = 200
        hist_dates = hist.index[-lookback:]
        plt.plot(hist_dates, hist['Close'].values[-lookback:], label='Historical', linewidth=1.5)
        plt.plot(business_days, forecast_results['mean'], label='Mean Forecast', linestyle='--')
        plt.fill_between(business_days, forecast_results['p5'], forecast_results['p95'], alpha=0.25, label='90% PI')
        plt.title(f"{TICKER} Forecast")
        plt.legend()
        plt.show()
    except Exception as e:
        print("‚ö†Ô∏è Plotting failed:", e)

if __name__ == "__main__":
    main()

import warnings
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from datetime import timedelta
from scipy.stats import norm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Robust import for GARCH model
try:
    from arch import arch_model
    HAS_ARCH = True
except ImportError:
    HAS_ARCH = False
    print("="*60)
    print("[WARNING] 'arch' library not found. GARCH volatility modeling will be skipped.")
    print("To enable GARCH, install it via: pip install arch")
    print("="*60)

# Suppress specific warnings for cleaner output
warnings.filterwarnings("ignore")

# ==========================================
# 1. CONFIGURATION
# ==========================================
TICKERS = ["OMN.JO"]

START_DATE = "2020-01-01"
TEST_SIZE_RATIO = 0.2  # Backtesting split (Last 20% for validation)

# SETTINGS FOR FUTURE FORECASTING
FUTURE_DAYS = 60
LAG_WINDOW = 5         # Past days used to predict the next day
MONTE_CARLO_SIMS = 200  # Increased sims for better distribution analysis

# ==========================================
# 2. DATA UTILITIES
# ==========================================
def get_stock_data(ticker):
    """Downloads historical stock data from Yahoo Finance."""
    print(f"\n[INFO] Downloading data for {ticker}...")
    try:
        df = yf.download(ticker, start=START_DATE, progress=False)

        # Handle multi-index columns if they exist
        if isinstance(df.columns, pd.MultiIndex):
            try:
                df = df.xs(ticker, level=1, axis=1)
            except:
                df.columns = df.columns.get_level_values(0)

        if df.empty:
            print(f"[ERROR] No data found for {ticker}. Check spelling or internet connection.")
            return None

        df = df[['Close']].dropna()
        return df
    except Exception as e:
        print(f"[ERROR] Failed to download {ticker}: {e}")
        return None

def train_test_split_series(series, test_ratio):
    split_idx = int(len(series) * (1 - test_ratio))
    return series.iloc[:split_idx], series.iloc[split_idx:]

def create_ml_dataset(series, window=5):
    """
    Revised to work with RETURNS, not raw prices.
    """
    X, y = [], []
    vals = series.values
    for i in range(len(vals) - window):
        X.append(vals[i:i+window])
        y.append(vals[i+window])
    return np.array(X), np.array(y)

def calculate_metrics(y_true, y_pred, model_name):
    """Calculates and prints RMSE and MAE."""
    # Align indices
    common_idx = y_true.index.intersection(y_pred.index)
    if len(common_idx) == 0:
        return np.nan, np.nan

    y_true_aligned = y_true.loc[common_idx]
    y_pred_aligned = y_pred.loc[common_idx]

    rmse = np.sqrt(mean_squared_error(y_true_aligned, y_pred_aligned))
    mae = mean_absolute_error(y_true_aligned, y_pred_aligned)

    print(f"  > {model_name} Performance:")
    print(f"    RMSE (Root Mean Sq Error): {rmse:.2f}")
    print(f"    MAE  (Mean Absolute Error): {mae:.2f}")
    return rmse, mae

# ==========================================
# 3. BACKTESTING MODELS (Past Performance)
# ==========================================

def run_arima_backtest(train, test, order=(5,1,0)):
    history = [x for x in train]
    predictions = []
    print("  > Backtesting ARIMA...")
    try:
        model = ARIMA(history, order=order)
        model_fit = model.fit()

        for t in range(len(test)):
            yhat = model_fit.forecast(steps=1)[0]
            predictions.append(yhat)
            history.append(test.iloc[t])
            model_fit = model_fit.append([test.iloc[t]], refit=False)

    except Exception as e:
        print(f"ARIMA Error: {e}")
        return pd.Series([np.nan]*len(test), index=test.index)
    return pd.Series(predictions, index=test.index)

def run_ml_backtest_returns(series, test_ratio, window):
    print("  > Backtesting ML Models (using Returns)...")
    log_returns = np.log(series / series.shift(1)).dropna()
    X, y = create_ml_dataset(log_returns, window)
    split_idx = int(len(X) * (1 - test_ratio))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train = y[:split_idx]

    gb = GradientBoostingRegressor(n_estimators=100, random_state=42)
    gb.fit(X_train, y_train)
    pred_returns = gb.predict(X_test)

    test_indices = log_returns.index[window + split_idx :]
    final_preds = []
    for i, date in enumerate(test_indices):
        prev_date_loc = series.index.get_loc(date) - 1
        prev_price = series.iloc[prev_date_loc]
        pred_price = prev_price * np.exp(pred_returns[i])
        final_preds.append(pred_price)

    results = {}
    results['Gradient Boosting'] = pd.Series(final_preds, index=test_indices)
    return results, gb

# ==========================================
# 4. FUTURE FORECASTING (Monte Carlo & ML)
# ==========================================

def run_monte_carlo(series, days, simulations=1000):
    print(f"  > Running Monte Carlo Simulation ({simulations} paths)...")
    log_returns = np.log(1 + series.pct_change())
    u = log_returns.mean()
    var = log_returns.var()
    drift = u - (0.5 * var)
    stdev = log_returns.std()

    last_price = series.iloc[-1]
    dates = [series.index[-1] + timedelta(days=i) for i in range(1, days + 1)]
    dates = [d for d in dates if d.weekday() < 5]
    steps = len(dates)

    simulation_df = pd.DataFrame(index=dates)
    final_prices = []

    for i in range(simulations):
        Z = norm.ppf(np.random.rand(steps))
        daily_returns = np.exp(drift + stdev * Z)
        price_path = np.zeros(steps)
        price_path[0] = last_price * daily_returns[0]
        for t in range(1, steps):
            price_path[t] = price_path[t-1] * daily_returns[t]

        simulation_df[f'Sim_{i}'] = price_path
        final_prices.append(price_path[-1])

    return simulation_df, final_prices

def run_future_forecasts(series, ml_model, steps=30):
    print(f"  > Generating Future Forecast for next {steps} days...")
    last_date = series.index[-1]
    future_dates = [last_date + timedelta(days=i) for i in range(1, steps + 1)]
    future_dates = [d for d in future_dates if d.weekday() < 5]
    actual_steps = len(future_dates)
    results = {}

    try:
        log_returns = np.log(series / series.shift(1)).dropna()
        current_window_ret = list(log_returns.values[-LAG_WINDOW:])
        current_price = series.iloc[-1]
        ml_preds_price = []
        for _ in range(actual_steps):
            input_feat = np.array([current_window_ret])
            pred_ret = ml_model.predict(input_feat)[0]
            next_price = current_price * np.exp(pred_ret)
            ml_preds_price.append(next_price)
            current_price = next_price
            current_window_ret.pop(0)
            current_window_ret.append(pred_ret)
        results['ML (Gradient Boosting)'] = pd.Series(ml_preds_price, index=future_dates)
    except: pass
    return results

def run_garch(series):
    if not HAS_ARCH: return None
    try:
        returns = 100 * series.pct_change().dropna()
        model = arch_model(returns, vol='Garch', p=1, q=1)
        res = model.fit(disp='off')
        return res.conditional_volatility
    except: return None

# ==========================================
# 5. MAIN LOOP
# ==========================================

def main():
    if not TICKERS:
        print("Please add tickers to the TICKERS list.")
        return

    for ticker in TICKERS:
        print("="*60)
        print(f"PROCESSING: {ticker}")
        print("="*60)

        df = get_stock_data(ticker)
        if df is None: continue

        series = df['Close']
        train, test = train_test_split_series(series, TEST_SIZE_RATIO)

        # --- PHASE 1: Backtesting & Validation ---
        bt_arima = run_arima_backtest(train, test)
        calculate_metrics(test, bt_arima, "ARIMA")

        bt_ml_results, trained_gb_model = run_ml_backtest_returns(series, TEST_SIZE_RATIO, LAG_WINDOW)
        calculate_metrics(test, bt_ml_results['Gradient Boosting'], "ML (Gradient Boosting)")

        volatility = run_garch(series)

        # --- PHASE 2: Future Forecasting & Probability ---
        future_results = run_future_forecasts(series, trained_gb_model, steps=FUTURE_DAYS)
        mc_sims, final_prices = run_monte_carlo(series, FUTURE_DAYS, MONTE_CARLO_SIMS)

        # Calculate Probability Statistics
        current_price = series.iloc[-1]
        mu, std = norm.fit(final_prices)

        print("\n  > PROBABILITY ANALYSIS (Next 60 Days):")
        print(f"    Current Price: {current_price:.2f}")
        print(f"    Expected Mean Price: {mu:.2f}")
        print(f"    Likelihood of Price Increase: {norm.sf(current_price, loc=mu, scale=std)*100:.1f}%")
        print(f"    95% Confidence Interval: {norm.ppf(0.025, mu, std):.2f} to {norm.ppf(0.975, mu, std):.2f}")

        # --- Visualization ---
        fig = plt.figure(figsize=(16, 8))
        gs = gridspec.GridSpec(1, 4) # 1 row, 4 columns

        # Main Price Chart (Takes up 3/4 of the space)
        ax1 = plt.subplot(gs[0, :3])

        # History & Actuals
        ax1.plot(train.index[-150:], train[-150:], label="History", color='gray', alpha=0.5)
        ax1.plot(test.index, test, label="Actual (Test)", color='black', linewidth=2)

        # Backtests
        if 'Gradient Boosting' in bt_ml_results:
            ax1.plot(bt_ml_results['Gradient Boosting'].index, bt_ml_results['Gradient Boosting'],
                     label="ML Backtest", linestyle=':', color='green', alpha=0.9)

        # Monte Carlo Cloud
        if not mc_sims.empty:
            ax1.plot(mc_sims.index, mc_sims, color='blue', alpha=0.05, linewidth=1)
            ax1.plot([], [], color='blue', alpha=0.3, label=f'Monte Carlo ({MONTE_CARLO_SIMS} Sims)')

        # Mean Forecast
        if 'ML (Gradient Boosting)' in future_results:
            ax1.plot(future_results['ML (Gradient Boosting)'].index, future_results['ML (Gradient Boosting)'],
                     label="ML Mean Forecast", color='red', linestyle='--', linewidth=2)

        ax1.set_title(f"Forecast: {ticker} (RMSE Validated)")
        ax1.legend(loc='upper left')
        ax1.grid(True, alpha=0.3)

        # Probability Density Chart (Takes up 1/4 of the space)
        ax2 = plt.subplot(gs[0, 3])

        # Create a Bell Curve (Normal Distribution) of Final Prices
        x_range = np.linspace(min(final_prices), max(final_prices), 100)
        p = norm.pdf(x_range, mu, std)

        # Plot Histogram and Bell Curve
        ax2.hist(final_prices, bins=15, density=True, alpha=0.6, color='blue', orientation='horizontal')
        ax2.plot(p, x_range, 'r', linewidth=2, label='Normal Dist')

        # Mark Current Price line
        ax2.axhline(y=current_price, color='black', linestyle='--', label='Current Price')

        ax2.set_title(f"Price Likelihood\n(in {FUTURE_DAYS} days)")
        ax2.set_xlabel("Probability Density")
        # ax2.get_yaxis().set_visible(False) # Share y-axis visually
        ax2.grid(True, alpha=0.3)
        ax2.legend(loc='upper right', fontsize='small')

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.show()
        print(f"Done. Analysis complete for {ticker}.\n")

if __name__ == "__main__":
    main()

import warnings
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from datetime import timedelta
from scipy.stats import norm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Robust import for GARCH model
try:
    from arch import arch_model
    HAS_ARCH = True
except ImportError:
    HAS_ARCH = False
    print("="*60)
    print("[WARNING] 'arch' library not found. GARCH volatility modeling will be skipped.")
    print("To enable GARCH, install it via: pip install arch")
    print("="*60)

warnings.filterwarnings("ignore")

# ==========================================
# 1. CONFIGURATION
# ==========================================
TICKERS = ["OMN.JO"]

START_DATE = "2018-01-01" # Extended history to capture better seasonality
TEST_SIZE_RATIO = 0.2

# SETTINGS FOR FUTURE FORECASTING
FUTURE_DAYS = 60
LAG_WINDOW = 5
MONTE_CARLO_SIMS = 200
SEASONAL_PERIOD = 60   # Detecting Quarterly patterns (approx 60 trading days)

# ==========================================
# 2. DATA UTILITIES
# ==========================================
def get_stock_data(ticker):
    print(f"\n[INFO] Downloading data for {ticker}...")
    try:
        df = yf.download(ticker, start=START_DATE, progress=False)
        if isinstance(df.columns, pd.MultiIndex):
            try:
                df = df.xs(ticker, level=1, axis=1)
            except:
                df.columns = df.columns.get_level_values(0)

        if df.empty:
            print(f"[ERROR] No data found for {ticker}.")
            return None

        df = df[['Close']].dropna()
        return df
    except Exception as e:
        print(f"[ERROR] Failed to download {ticker}: {e}")
        return None

def train_test_split_series(series, test_ratio):
    split_idx = int(len(series) * (1 - test_ratio))
    return series.iloc[:split_idx], series.iloc[split_idx:]

# ==========================================
# 3. FEATURE ENGINEERING (SEASONALITY)
# ==========================================
def create_seasonal_features(df, window=5):
    """
    Creates a rich feature set capturing Seasonality and Trends.
    Returns: X (Features), y (Target Log Returns), scaler references
    """
    data = df.copy()

    # 1. Target: Log Returns (Stationary)
    data['Log_Ret'] = np.log(data['Close'] / data['Close'].shift(1))

    # 2. Lag Features (Short-term Momentum)
    for i in range(1, window + 1):
        data[f'Lag_{i}'] = data['Log_Ret'].shift(i)

    # 3. Trend Features (Moving Averages of Returns)
    data['MA_20'] = data['Log_Ret'].rolling(window=20).mean() # Monthly Trend
    data['MA_60'] = data['Log_Ret'].rolling(window=60).mean() # Quarterly Trend

    # 4. Seasonal/Calendar Features (Cyclical Encoding)
    # We map Month (1-12) to a circle using Sin/Cos so Dec(12) is close to Jan(1)
    data['Month_Sin'] = np.sin(2 * np.pi * data.index.month / 12)
    data['Month_Cos'] = np.cos(2 * np.pi * data.index.month / 12)

    data = data.dropna()

    feature_cols = [f'Lag_{i}' for i in range(1, window + 1)] + \
                   ['MA_20', 'MA_60', 'Month_Sin', 'Month_Cos']

    X = data[feature_cols].values
    y = data['Log_Ret'].values

    return X, y, data[feature_cols].columns, data

# ==========================================
# 4. MODELING & FORECASTING
# ==========================================

def calculate_metrics(y_true, y_pred, model_name):
    # Align indices
    common_idx = y_true.index.intersection(y_pred.index)
    if len(common_idx) == 0: return np.nan, np.nan

    y_true = y_true.loc[common_idx]
    y_pred = y_pred.loc[common_idx]

    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)

    print(f"  > {model_name} Performance:")
    print(f"    RMSE: {rmse:.2f}")
    print(f"    MAE : {mae:.2f}")
    return rmse, mae

def run_ml_seasonal_backtest(series, test_ratio, window):
    print("  > Training ML Model with Seasonal Features...")

    # Convert Series to DataFrame for Feature Engineering
    df_input = series.to_frame(name='Close')
    X, y, feature_names, processed_df = create_seasonal_features(df_input, window)

    # Split based on ratio
    split_idx = int(len(X) * (1 - test_ratio))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train = y[:split_idx]

    # Gradient Boosting (Handles non-linear seasonality well)
    model = GradientBoostingRegressor(n_estimators=150, learning_rate=0.05, max_depth=4, random_state=42)
    model.fit(X_train, y_train)

    # Predict Returns
    pred_returns = model.predict(X_test)

    # Reconstruct Prices
    test_dates = processed_df.index[split_idx:]
    reconstructed_prices = []

    # Anchor to the price just before test set
    last_val_idx = processed_df.index.get_loc(test_dates[0]) - 1
    last_price = processed_df['Close'].iloc[last_val_idx]

    # Dynamic Reconstruction (using predicted returns on previous reconstructed price for full path)
    # Note: For strict 1-step accuracy check, we'd use previous ACTUAL price.
    # But to see if model captures "shape/seasonality", we can let it run freely or 1-step.
    # We will use 1-step update for Backtest Accuracy:
    for i, date in enumerate(test_dates):
        # Previous ACTUAL price
        prev_price_loc = df_input.index.get_loc(date) - 1
        prev_actual = df_input['Close'].iloc[prev_price_loc]
        pred_p = prev_actual * np.exp(pred_returns[i])
        reconstructed_prices.append(pred_p)

    results = pd.Series(reconstructed_prices, index=test_dates)
    return results, model, processed_df

def run_future_forecast_seasonal(series, model, window, steps=60):
    print(f"  > Generating Future Forecast ({steps} days) with Seasonality...")

    # We need to rebuild the 'processed_df' state to get the latest moving averages
    df_input = series.to_frame(name='Close')
    _, _, feature_names, processed_data = create_seasonal_features(df_input, window)

    # Initial State (Last known values)
    last_row = processed_data.iloc[-1]
    last_close = processed_data['Close'].iloc[-1]

    # Buffer for calculating rolling means dynamically
    # We need the last 60 returns to calculate MA_60
    recent_returns = list(processed_data['Log_Ret'].values[-60:])

    future_dates = [series.index[-1] + timedelta(days=i) for i in range(1, steps * 2)] # Generate extra, filter weekends
    future_dates = [d for d in future_dates if d.weekday() < 5][:steps]

    future_prices = []

    for date in future_dates:
        # 1. Build Feature Vector for Today
        # Lags: recent_returns[-1], recent_returns[-2]...
        lags = [recent_returns[-i] for i in range(1, window + 1)]

        # Rolling Means
        ma_20 = np.mean(recent_returns[-20:])
        ma_60 = np.mean(recent_returns[-60:])

        # Seasonality
        mon_sin = np.sin(2 * np.pi * date.month / 12)
        mon_cos = np.cos(2 * np.pi * date.month / 12)

        features = np.array(lags + [ma_20, ma_60, mon_sin, mon_cos]).reshape(1, -1)

        # 2. Predict Next Return
        pred_ret = model.predict(features)[0]

        # 3. Update Price
        next_price = last_close * np.exp(pred_ret)
        future_prices.append(next_price)

        # 4. Update State for next loop
        recent_returns.append(pred_ret)
        recent_returns.pop(0) # Keep buffer size constant-ish (or just append, doesn't matter for slicing)
        last_close = next_price

    return pd.Series(future_prices, index=future_dates)

def run_monte_carlo(series, days, simulations=200):
    print(f"  > Running Monte Carlo Simulation...")
    log_returns = np.log(1 + series.pct_change())
    u = log_returns.mean()
    var = log_returns.var()
    drift = u - (0.5 * var)
    stdev = log_returns.std()

    last_price = series.iloc[-1]
    dates = [series.index[-1] + timedelta(days=i) for i in range(1, int(days*1.5))]
    dates = [d for d in dates if d.weekday() < 5][:days]
    steps = len(dates)

    simulation_df = pd.DataFrame(index=dates)
    final_prices = []

    for i in range(simulations):
        Z = norm.ppf(np.random.rand(steps))
        daily_returns = np.exp(drift + stdev * Z)
        price_path = np.zeros(steps)
        price_path[0] = last_price * daily_returns[0]
        for t in range(1, steps):
            price_path[t] = price_path[t-1] * daily_returns[t]

        simulation_df[f'Sim_{i}'] = price_path
        final_prices.append(price_path[-1])

    return simulation_df, final_prices

# ==========================================
# 5. MAIN LOOP
# ==========================================
def main():
    if not TICKERS:
        print("Please add tickers to the TICKERS list.")
        return

    for ticker in TICKERS:
        print("="*60)
        print(f"PROCESSING: {ticker}")
        print("="*60)

        df = get_stock_data(ticker)
        if df is None: continue

        series = df['Close']
        train, test = train_test_split_series(series, TEST_SIZE_RATIO)

        # --- 1. Seasonal Decomposition (Visual Check) ---
        # Decompose the last 2 years to see recent seasonality
        decomp_data = series.iloc[-500:]
        decomposition = seasonal_decompose(decomp_data, model='multiplicative', period=SEASONAL_PERIOD)

        # --- 2. ML Training & Backtest ---
        bt_results, ml_model, _ = run_ml_seasonal_backtest(series, TEST_SIZE_RATIO, LAG_WINDOW)
        calculate_metrics(test, bt_results, "Seasonal ML Model")

        # --- 3. Future Forecast ---
        future_series = run_future_forecast_seasonal(series, ml_model, LAG_WINDOW, steps=FUTURE_DAYS)
        mc_sims, final_prices = run_monte_carlo(series, FUTURE_DAYS, MONTE_CARLO_SIMS)

        # Statistics
        current_price = series.iloc[-1]
        mu, std = norm.fit(final_prices)

        print("\n  > PROBABILITY ANALYSIS:")
        print(f"    Current: {current_price:.2f} | Exp. Mean: {mu:.2f}")
        print(f"    Prob. Increase: {norm.sf(current_price, loc=mu, scale=std)*100:.1f}%")

        # --- VISUALIZATION ---
        fig = plt.figure(figsize=(16, 12))
        gs = gridspec.GridSpec(3, 2)

        # Top Row: Main Forecast
        ax1 = plt.subplot(gs[0, :])
        ax1.plot(train.index[-200:], train[-200:], label="History", color='gray', alpha=0.5)
        ax1.plot(test.index, test, label="Actual", color='black', linewidth=2)
        ax1.plot(bt_results.index, bt_results, label="Backtest (Seasonal)", color='green', linestyle=':', alpha=0.8)

        if not mc_sims.empty:
             ax1.plot(mc_sims.index, mc_sims, color='blue', alpha=0.05, linewidth=1)
        ax1.plot(future_series.index, future_series, label="Future Trend (Seasonal)", color='red', linestyle='--', linewidth=2)

        ax1.set_title(f"{ticker} Forecast with Seasonal Factors")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Middle Row Left: Trend Component
        ax2 = plt.subplot(gs[1, 0])
        ax2.plot(decomposition.trend, color='purple')
        ax2.set_title("Underlying Trend (Decomposed)")
        ax2.grid(True, alpha=0.3)

        # Middle Row Right: Seasonal Component
        ax3 = plt.subplot(gs[1, 1])
        ax3.plot(decomposition.seasonal, color='teal')
        ax3.set_title(f"Detected Seasonality (Period={SEASONAL_PERIOD} days)")
        ax3.grid(True, alpha=0.3)

        # Bottom Row: Probability Dist
        ax4 = plt.subplot(gs[2, :])
        x_range = np.linspace(min(final_prices), max(final_prices), 100)
        p = norm.pdf(x_range, mu, std)
        ax4.hist(final_prices, bins=20, density=True, alpha=0.6, color='blue')
        ax4.plot(x_range, p, 'r', linewidth=2)
        ax4.axvline(x=current_price, color='black', linestyle='--', label='Current Price')
        ax4.set_title("Future Price Probability Distribution")
        ax4.legend()

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.show()
        print(f"Done. Analysis complete for {ticker}.\n")

if __name__ == "__main__":
    main()

import warnings
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from datetime import timedelta
from scipy.stats import norm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Robust import for GARCH model
try:
    from arch import arch_model
    HAS_ARCH = True
except ImportError:
    HAS_ARCH = False
    print("="*60)
    print("[WARNING] 'arch' library not found. GARCH volatility modeling will be skipped.")
    print("To enable GARCH, install it via: pip install arch")
    print("="*60)

warnings.filterwarnings("ignore")

# ==========================================
# 1. CONFIGURATION
# ==========================================
TICKERS = ["IMP.JO"]

START_DATE = "2018-01-01" # Extended history to capture better seasonality
TEST_SIZE_RATIO = 0.2

# SETTINGS FOR FUTURE FORECASTING
FUTURE_DAYS = 60
LAG_WINDOW = 5
MONTE_CARLO_SIMS = 200
SEASONAL_PERIOD = 60   # Detecting Quarterly patterns (approx 60 trading days)

# ==========================================
# 2. DATA UTILITIES
# ==========================================
def get_stock_data(ticker):
    print(f"\n[INFO] Downloading data for {ticker}...")
    try:
        df = yf.download(ticker, start=START_DATE, progress=False)
        if isinstance(df.columns, pd.MultiIndex):
            try:
                df = df.xs(ticker, level=1, axis=1)
            except:
                df.columns = df.columns.get_level_values(0)

        if df.empty:
            print(f"[ERROR] No data found for {ticker}.")
            return None

        df = df[['Close']].dropna()
        return df
    except Exception as e:
        print(f"[ERROR] Failed to download {ticker}: {e}")
        return None

def train_test_split_series(series, test_ratio):
    split_idx = int(len(series) * (1 - test_ratio))
    return series.iloc[:split_idx], series.iloc[split_idx:]

# ==========================================
# 3. FEATURE ENGINEERING (SEASONALITY)
# ==========================================
def create_seasonal_features(df, window=5):
    """
    Creates a rich feature set capturing Seasonality and Trends.
    Returns: X (Features), y (Target Log Returns), scaler references
    """
    data = df.copy()

    # 1. Target: Log Returns (Stationary)
    data['Log_Ret'] = np.log(data['Close'] / data['Close'].shift(1))

    # 2. Lag Features (Short-term Momentum)
    for i in range(1, window + 1):
        data[f'Lag_{i}'] = data['Log_Ret'].shift(i)

    # 3. Trend Features (Moving Averages of Returns)
    data['MA_20'] = data['Log_Ret'].rolling(window=20).mean() # Monthly Trend
    data['MA_60'] = data['Log_Ret'].rolling(window=60).mean() # Quarterly Trend

    # 4. Seasonal/Calendar Features (Cyclical Encoding)
    # We map Month (1-12) to a circle using Sin/Cos so Dec(12) is close to Jan(1)
    data['Month_Sin'] = np.sin(2 * np.pi * data.index.month / 12)
    data['Month_Cos'] = np.cos(2 * np.pi * data.index.month / 12)

    data = data.dropna()

    feature_cols = [f'Lag_{i}' for i in range(1, window + 1)] + \
                   ['MA_20', 'MA_60', 'Month_Sin', 'Month_Cos']

    X = data[feature_cols].values
    y = data['Log_Ret'].values

    return X, y, data[feature_cols].columns, data

# ==========================================
# 4. MODELING & FORECASTING
# ==========================================

def calculate_metrics(y_true, y_pred, model_name):
    # Align indices
    common_idx = y_true.index.intersection(y_pred.index)
    if len(common_idx) == 0: return np.nan, np.nan

    y_true = y_true.loc[common_idx]
    y_pred = y_pred.loc[common_idx]

    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)

    print(f"  > {model_name} Performance:")
    print(f"    RMSE: {rmse:.2f}")
    print(f"    MAE : {mae:.2f}")
    return rmse, mae

def run_ml_seasonal_backtest(series, test_ratio, window):
    print("  > Training ML Model with Seasonal Features...")

    # Convert Series to DataFrame for Feature Engineering
    df_input = series.to_frame(name='Close')
    X, y, feature_names, processed_df = create_seasonal_features(df_input, window)

    # Split based on ratio
    split_idx = int(len(X) * (1 - test_ratio))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train = y[:split_idx]

    # Gradient Boosting (Handles non-linear seasonality well)
    model = GradientBoostingRegressor(n_estimators=150, learning_rate=0.05, max_depth=4, random_state=42)
    model.fit(X_train, y_train)

    # Predict Returns
    pred_returns = model.predict(X_test)

    # Reconstruct Prices
    test_dates = processed_df.index[split_idx:]
    reconstructed_prices = []

    # Anchor to the price just before test set
    last_val_idx = processed_df.index.get_loc(test_dates[0]) - 1
    last_price = processed_df['Close'].iloc[last_val_idx]

    # Dynamic Reconstruction (using predicted returns on previous reconstructed price for full path)
    for i, date in enumerate(test_dates):
        # Previous ACTUAL price
        prev_price_loc = df_input.index.get_loc(date) - 1
        prev_actual = df_input['Close'].iloc[prev_price_loc]
        pred_p = prev_actual * np.exp(pred_returns[i])
        reconstructed_prices.append(pred_p)

    results = pd.Series(reconstructed_prices, index=test_dates)
    return results, model, processed_df

def run_future_forecast_seasonal(series, model, window, steps=60):
    print(f"  > Generating Future Forecast ({steps} days) with Seasonality...")

    # We need to rebuild the 'processed_df' state to get the latest moving averages
    df_input = series.to_frame(name='Close')
    _, _, feature_names, processed_data = create_seasonal_features(df_input, window)

    # Initial State (Last known values)
    last_close = processed_data['Close'].iloc[-1]

    # Buffer for calculating rolling means dynamically
    # We need the last 60 returns to calculate MA_60
    recent_returns = list(processed_data['Log_Ret'].values[-60:])

    future_dates = [series.index[-1] + timedelta(days=i) for i in range(1, steps * 2)] # Generate extra, filter weekends
    future_dates = [d for d in future_dates if d.weekday() < 5][:steps]

    future_prices = []

    for date in future_dates:
        # 1. Build Feature Vector for Today
        # Lags: recent_returns[-1], recent_returns[-2]...
        lags = [recent_returns[-i] for i in range(1, window + 1)]

        # Rolling Means
        ma_20 = np.mean(recent_returns[-20:])
        ma_60 = np.mean(recent_returns[-60:])

        # Seasonality
        mon_sin = np.sin(2 * np.pi * date.month / 12)
        mon_cos = np.cos(2 * np.pi * date.month / 12)

        features = np.array(lags + [ma_20, ma_60, mon_sin, mon_cos]).reshape(1, -1)

        # 2. Predict Next Return
        pred_ret = model.predict(features)[0]

        # 3. Update Price
        next_price = last_close * np.exp(pred_ret)
        future_prices.append(next_price)

        # 4. Update State for next loop
        recent_returns.append(pred_ret)
        recent_returns.pop(0)
        last_close = next_price

    return pd.Series(future_prices, index=future_dates)

def forecast_underlying_trend(trend_series, steps=60):
    """
    Forecasts the decomposed trend using Holt's Linear Method (Exponential Smoothing).
    """
    # Trend decomposition often leaves NaNs at start/end, remove them
    clean_trend = trend_series.dropna()

    try:
        # Holt's Linear Trend method (Double Exponential Smoothing)
        # 'add' assumes a linear trend, which is standard for short term projection of smoothed data
        model = ExponentialSmoothing(clean_trend, trend='add', seasonal=None, initialization_method="estimated")
        fit_model = model.fit()

        # Generate future dates
        last_date = clean_trend.index[-1]
        future_dates = [last_date + timedelta(days=i) for i in range(1, steps * 2)]
        future_dates = [d for d in future_dates if d.weekday() < 5][:steps]

        forecast_values = fit_model.forecast(len(future_dates))
        return pd.Series(forecast_values.values, index=future_dates)
    except Exception as e:
        print(f"  [Error] Trend forecast failed: {e}")
        return pd.Series()

def run_monte_carlo(series, days, simulations=200):
    print(f"  > Running Monte Carlo Simulation...")
    log_returns = np.log(1 + series.pct_change())
    u = log_returns.mean()
    var = log_returns.var()
    drift = u - (0.5 * var)
    stdev = log_returns.std()

    last_price = series.iloc[-1]
    dates = [series.index[-1] + timedelta(days=i) for i in range(1, int(days*1.5))]
    dates = [d for d in dates if d.weekday() < 5][:days]
    steps = len(dates)

    simulation_df = pd.DataFrame(index=dates)
    final_prices = []

    for i in range(simulations):
        Z = norm.ppf(np.random.rand(steps))
        daily_returns = np.exp(drift + stdev * Z)
        price_path = np.zeros(steps)
        price_path[0] = last_price * daily_returns[0]
        for t in range(1, steps):
            price_path[t] = price_path[t-1] * daily_returns[t]

        simulation_df[f'Sim_{i}'] = price_path
        final_prices.append(price_path[-1])

    return simulation_df, final_prices

# ==========================================
# 5. MAIN LOOP
# ==========================================
def main():
    if not TICKERS:
        print("Please add tickers to the TICKERS list.")
        return

    for ticker in TICKERS:
        print("="*60)
        print(f"PROCESSING: {ticker}")
        print("="*60)

        df = get_stock_data(ticker)
        if df is None: continue

        series = df['Close']
        train, test = train_test_split_series(series, TEST_SIZE_RATIO)

        # --- 1. Seasonal Decomposition & Trend Forecast ---
        # Decompose the last 2 years to see recent seasonality
        decomp_data = series.iloc[-500:]
        decomposition = seasonal_decompose(decomp_data, model='multiplicative', period=SEASONAL_PERIOD)

        # NEW: Predict where the trend is going
        trend_forecast = forecast_underlying_trend(decomposition.trend, steps=FUTURE_DAYS)

        # --- 2. ML Training & Backtest ---
        bt_results, ml_model, _ = run_ml_seasonal_backtest(series, TEST_SIZE_RATIO, LAG_WINDOW)
        calculate_metrics(test, bt_results, "Seasonal ML Model")

        # --- 3. Future Forecast ---
        future_series = run_future_forecast_seasonal(series, ml_model, LAG_WINDOW, steps=FUTURE_DAYS)
        mc_sims, final_prices = run_monte_carlo(series, FUTURE_DAYS, MONTE_CARLO_SIMS)

        # Statistics
        current_price = series.iloc[-1]
        mu, std = norm.fit(final_prices)

        print("\n  > PROBABILITY ANALYSIS:")
        print(f"    Current: {current_price:.2f} | Exp. Mean: {mu:.2f}")
        print(f"    Prob. Increase: {norm.sf(current_price, loc=mu, scale=std)*100:.1f}%")

        # --- VISUALIZATION ---
        fig = plt.figure(figsize=(16, 12))
        gs = gridspec.GridSpec(3, 2)

        # Top Row: Main Forecast
        ax1 = plt.subplot(gs[0, :])
        ax1.plot(train.index[-200:], train[-200:], label="History", color='gray', alpha=0.5)
        ax1.plot(test.index, test, label="Actual", color='black', linewidth=2)
        ax1.plot(bt_results.index, bt_results, label="Backtest (Seasonal)", color='green', linestyle=':', alpha=0.8)

        if not mc_sims.empty:
             ax1.plot(mc_sims.index, mc_sims, color='blue', alpha=0.05, linewidth=1)
        ax1.plot(future_series.index, future_series, label="Future Trend (Seasonal)", color='red', linestyle='--', linewidth=2)

        ax1.set_title(f"{ticker} Forecast with Seasonal Factors")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Middle Row Left: Trend Component + Forecast
        ax2 = plt.subplot(gs[1, 0])
        ax2.plot(decomposition.trend, color='purple', label='Historical Trend')
        if not trend_forecast.empty:
            ax2.plot(trend_forecast.index, trend_forecast, color='red', linestyle='--', linewidth=2, label='Trend Forecast (Holt)')
        ax2.set_title("Underlying Trend (Decomposed + 60 Day Forecast)")
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # Middle Row Right: Seasonal Component
        ax3 = plt.subplot(gs[1, 1])
        ax3.plot(decomposition.seasonal, color='teal')
        ax3.set_title(f"Detected Seasonality (Period={SEASONAL_PERIOD} days)")
        ax3.grid(True, alpha=0.3)

        # Bottom Row: Probability Dist
        ax4 = plt.subplot(gs[2, :])
        x_range = np.linspace(min(final_prices), max(final_prices), 100)
        p = norm.pdf(x_range, mu, std)
        ax4.hist(final_prices, bins=20, density=True, alpha=0.6, color='blue')
        ax4.plot(x_range, p, 'r', linewidth=2)
        ax4.axvline(x=current_price, color='black', linestyle='--', label='Current Price')
        ax4.set_title("Future Price Probability Distribution")
        ax4.legend()

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.show()
        print(f"Done. Analysis complete for {ticker}.\n")

if __name__ == "__main__":
    main()

import warnings
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

warnings.filterwarnings("ignore")

# ==========================================
# 1. CONFIGURATION
# ==========================================
# We focus on one stock at a time to see its specific "Equation"
TARGET_STOCK = "STXSHA.JO"

# MACRO-ECONOMIC FACTORS (The "Independent Variables")
# SOUTH AFRICAN CONTEXT UPDATED
MACRO_TICKERS = {
    'JSE_Top40': 'STX40.JO',  # Satrix Top 40 ETF (Proxy for J200 Market Sentiment)
    'USD_ZAR': 'ZAR=X',       # Rand Weakness/Strength (Crucial for Miners)
    'Gold': 'GC=F',           # Commodity Hedge
    'Oil': 'CL=F',            # Inflation / Energy Input Costs
    'SA_Bonds': 'STXGOV.JO',  # Satrix SA Bond ETF (Proxy for Local Cost of Capital)
                              # NOTE: Price is INVERSE to Yields.
                              # Price UP = Yields DOWN. Price DOWN = Yields UP.
    'Platinum': 'PL=F'        # Specific Micro-factor for Impala
}

START_DATE = "2020-01-01"

# ==========================================
# 2. DATA ACQUISITION & MERGING
# ==========================================
def fetch_data(target, macro_dict):
    print(f"[INFO] Fetching data for Target: {target}...")

    # 1. Fetch Target Stock
    try:
        df_target = yf.download(target, start=START_DATE, progress=False)
        # Handle multi-index if present
        if isinstance(df_target.columns, pd.MultiIndex):
            df_target = df_target.xs(target, level=1, axis=1)
        df_target = df_target[['Close']].rename(columns={'Close': 'Target_Price'})
    except Exception as e:
        print(f"[ERROR] Could not fetch target: {e}")
        return None

    # 2. Fetch Macro Factors
    macro_data = []
    for name, ticker in macro_dict.items():
        print(f"       Fetching Macro Factor: {name} ({ticker})...")
        try:
            df = yf.download(ticker, start=START_DATE, progress=False)
            if isinstance(df.columns, pd.MultiIndex):
                try:
                    df = df.xs(ticker, level=1, axis=1)
                except:
                    df.columns = df.columns.get_level_values(0)

            # Use 'Close' price
            series = df['Close'].rename(name)
            macro_data.append(series)
        except:
            print(f"       [WARN] Failed to fetch {name}")

    # 3. Merge Everything
    print("[INFO] Merging Datasets and aligning dates...")
    df_macro = pd.concat(macro_data, axis=1)

    # Inner join to ensure we only use days where we have ALL data (Stock + Macro)
    full_df = pd.concat([df_target, df_macro], axis=1).fillna(method='ffill').dropna()

    return full_df

# ==========================================
# 3. REGRESSION MODELING
# ==========================================
def train_multilinear_model(df):
    # Define Predictors (X) and Target (y)
    features = [col for col in df.columns if col != 'Target_Price']
    X = df[features]
    y = df['Target_Price']

    # Split Data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

    # Initialize and Train
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Predictions
    preds = model.predict(X_test)

    # Score
    r2 = r2_score(y_test, preds)
    rmse = np.sqrt(mean_squared_error(y_test, preds))

    return model, features, X_test, y_test, preds, r2, rmse

# ==========================================
# 4. SCENARIO ANALYSIS (PREDICTION)
# ==========================================
def predict_scenario(model, features, current_values):
    """
    Predicts stock price based on HYPOTHETICAL future macro conditions.
    """
    # Create a dataframe for the scenario
    scenario_df = pd.DataFrame([current_values], columns=features)
    predicted_price = model.predict(scenario_df)[0]
    return predicted_price

# ==========================================
# 5. MAIN EXECUTION
# ==========================================
def main():
    # 1. Get Data
    df = fetch_data(TARGET_STOCK, MACRO_TICKERS)
    if df is None or len(df) < 50:
        print("Not enough data to run regression.")
        return

    # 2. Run Regression
    model, feature_names, X_test, y_test, preds, r2, rmse = train_multilinear_model(df)

    # 3. Output Statistics
    print("\n" + "="*50)
    print(f"ECONOMETRIC MODEL RESULTS (SA CONTEXT): {TARGET_STOCK}")
    print("="*50)
    print(f"Model Accuracy (R-Squared): {r2:.2f} (1.0 is perfect)")
    print(f"Average Error (RMSE): {rmse:.2f}")

    print("\n--- THE MARKET EQUATION ---")
    print(f"Base Price (Intercept): {model.intercept_:.2f}")
    print("Factor Coefficients (Sensitivity):")

    coef_data = []
    for name, coef in zip(feature_names, model.coef_):
        # Interpret: For every 1 unit increase in Factor, Stock moves by Coef
        print(f"  > {name:<10}: {coef:+.4f}")
        coef_data.append({'Factor': name, 'Coefficient': coef})

    # 4. Scenario Prediction (What if...?)
    latest_macro = df.iloc[-1][feature_names].to_dict()

    print("\n--- SCENARIO ANALYSIS ---")
    print("Current Macro Conditions:")
    for k, v in latest_macro.items():
        print(f"  {k}: {v:.2f}")

    fair_value = predict_scenario(model, feature_names, list(latest_macro.values()))
    actual_price = df.iloc[-1]['Target_Price']

    print(f"\nModel 'Fair Value' Calculation: {fair_value:.2f}")
    print(f"Actual Current Price:           {actual_price:.2f}")
    diff = ((actual_price - fair_value) / fair_value) * 100
    if diff > 0:
        print(f"VERDICT: Stock is potentially OVERVALUED by {diff:.1f}% vs Macro Fundamentals")
    else:
        print(f"VERDICT: Stock is potentially UNDERVALUED by {abs(diff):.1f}% vs Macro Fundamentals")

    # 5. Visualization
    plt.figure(figsize=(15, 10))

    # Plot A: Regression Fit (Actual vs Predicted)
    plt.subplot(2, 2, 1)
    plt.plot(y_test.index, y_test, label='Actual Price', color='black')
    plt.plot(y_test.index, preds, label='Model Fair Value', color='blue', linestyle='--')
    plt.title(f"Model Fit: Does the SA Economy explain {TARGET_STOCK}?")
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Plot B: Correlation Matrix
    plt.subplot(2, 2, 2)
    corr = df.corr()
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
    plt.title("Correlation Matrix: SA Market Factors")

    # Plot C: Coefficient Impact
    plt.subplot(2, 1, 2)
    coef_df = pd.DataFrame(coef_data)
    coef_df['Abs_Coef'] = coef_df['Coefficient'].abs()
    coef_df = coef_df.sort_values('Abs_Coef', ascending=False)

    colors = ['green' if x > 0 else 'red' for x in coef_df['Coefficient']]
    plt.bar(coef_df['Factor'], coef_df['Coefficient'], color=colors)
    plt.axhline(0, color='black', linewidth=1)
    plt.title(f"Factor Sensitivity: What drives {TARGET_STOCK}?")
    plt.ylabel("Impact (Coefficient)")
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()

import warnings
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import logging
import os
import sys
from datetime import datetime, timedelta

# ==========================================
# 0. SELF-HEALING ENVIRONMENT SETUP
# ==========================================
def fix_prophet_backend():
    """
    Attempts to automatically fix the missing C++ backend for Prophet.
    """
    try:
        print("[SETUP] Checking Prophet backend (CmdStanPy)...")
        import cmdstanpy
        from cmdstanpy import install_cmdstan

        # Check if cmdstan is installed
        try:
            cmdstan_path = cmdstanpy.cmdstan_path()
            print(f"[SETUP] CmdStan found at: {cmdstan_path}")
        except ValueError:
            print("[SETUP] CmdStan not found. Installing now (this takes 1-2 mins)...")
            install_cmdstan()
            print("[SETUP] Installation complete.")

    except ImportError:
        print("[ERROR] cmdstanpy not installed. Please run: pip install cmdstanpy")
    except Exception as e:
        print(f"[WARNING] Could not auto-fix backend: {e}")

# Run the fix before importing Prophet
fix_prophet_backend()

# ==========================================
# PROPHET IMPORT & LOGGING FIX
# ==========================================
logger = logging.getLogger('cmdstanpy')
logger.addHandler(logging.NullHandler())
logger.propagate = False
logger.setLevel(logging.CRITICAL)
logging.getLogger('prophet').setLevel(logging.WARNING)

try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    print("="*60)
    print("[ERROR] 'prophet' library not found.")
    print("Please install it using: pip install prophet")
    print("="*60)

warnings.filterwarnings("ignore")

# ==========================================
# 1. CONFIGURATION
# ==========================================
# CORRECTED TICKERS for JSE
TICKERS = ["STXSHA.JO"] # Aspen Pharmacare & Impala Platinum
START_DATE = "2018-01-01"
FORECAST_DAYS = 60

# ==========================================
# 2. DATA UTILITIES
# ==========================================
def get_data_for_prophet(ticker):
    print(f"\n[INFO] Downloading data for {ticker}...")
    try:
        df = yf.download(ticker, start=START_DATE, progress=False)

        if df.empty:
            print(f"[WARNING] No data downloaded for {ticker}. Check ticker symbol.")
            return None

        if isinstance(df.columns, pd.MultiIndex):
            try:
                df = df.xs(ticker, level=1, axis=1)
            except:
                df.columns = df.columns.get_level_values(0)

        # Prophet Formatting: ds (Date) and y (Value)
        df_prophet = df[['Close']].reset_index()
        df_prophet.columns = ['ds', 'y']
        df_prophet['ds'] = df_prophet['ds'].dt.tz_localize(None)

        return df_prophet
    except Exception as e:
        print(f"[ERROR] Failed to download {ticker}: {e}")
        return None

# ==========================================
# 3. PROPHET MODELING ENGINE
# ==========================================
def run_prophet_forecast(df, ticker):
    print(f"  > Training Prophet Model on {ticker}...")

    try:
        model = Prophet(
            daily_seasonality=True,
            yearly_seasonality=True,
            weekly_seasonality=True,
            changepoint_prior_scale=0.05
        )

        # Add SA Holidays
        try:
            model.add_country_holidays(country_name='ZA')
        except:
            print("    [!] Warning: Could not load SA holidays.")

        model.fit(df)

        future = model.make_future_dataframe(periods=FORECAST_DAYS)
        # Filter weekends
        future['day'] = future['ds'].dt.dayofweek
        future = future[future['day'] < 5]

        forecast = model.predict(future)
        return model, forecast

    except Exception as e:
        print(f"\n[CRITICAL ERROR] Prophet failed: {e}")
        print("Suggestion: Try restarting your runtime/kernel if the auto-fix ran.")
        return None, None

# ==========================================
# 4. VISUALIZATION & ANALYSIS
# ==========================================
def analyze_forecast(forecast, current_price):
    if forecast is None: return

    future_data = forecast.iloc[-FORECAST_DAYS:]
    start_future = future_data.iloc[0]['yhat']
    end_future = future_data.iloc[-1]['yhat']
    trend_pct = ((end_future - start_future) / start_future) * 100

    print("\n" + "-"*40)
    print(f"PROPHET FORECAST REPORT (Next {FORECAST_DAYS} Days)")
    print("-" * 40)
    print(f"Current Price (Approx): {current_price:.2f}")
    print(f"Predicted End Price:    {end_future:.2f}")
    print(f"Expected Trend:         {trend_pct:+.2f}%")

    latest = forecast.iloc[-1]
    print("\nDrivers of this movement:")
    print(f"  > Underlying Trend:     {latest['trend']:.2f}")
    print(f"  > Weekly Seasonality:   {latest['weekly']:.2f}")
    print(f"  > Yearly Seasonality:   {latest['yearly']:.2f}")

def plot_prophet_results(model, forecast, ticker):
    if model is None or forecast is None: return

    # Main Plot
    fig1 = model.plot(forecast)
    plt.title(f"{ticker} - Prophet Forecast")
    plt.xlabel("Date")
    plt.ylabel("Price (ZAR)")
    plt.grid(True, alpha=0.3)

    # Components Plot
    fig2 = model.plot_components(forecast)
    plt.suptitle(f"{ticker} - Seasonal Decomposition", y=1.02)
    plt.show()

# ==========================================
# 5. MAIN LOOP
# ==========================================
def main():
    if not PROPHET_AVAILABLE:
        print("Prophet library not found. Please install it.")
        return

    for ticker in TICKERS:
        print("\n" + "="*60)
        print(f"PROCESSING: {ticker}")
        print("="*60)

        df = get_data_for_prophet(ticker)
        if df is None: continue

        current_price = df.iloc[-1]['y']

        model, forecast = run_prophet_forecast(df, ticker)

        if model is not None:
            analyze_forecast(forecast, current_price)
            plot_prophet_results(model, forecast, ticker)
            print(f"Done. Prophet analysis complete for {ticker}.")

if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np
import yfinance as yf
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D # For 3D plotting
import warnings

# Suppress the FutureWarning from yfinance to keep the output clean
warnings.filterwarnings('ignore', category=FutureWarning)

# --- Configuration ---
# Default stocks: JSE listed stocks (expanded list)
STOCKS = ['IMP.JO', 'APN.JO', 'BHG.JO', 'AGL.JO', 'SOL.JO','GFI.JO','ANG.JO','VAL.JO','NPH.JO','CLS.JO','MRP.JO','MTN.JO','VOD.JO','OMN.JO','SDO.JO','RFG.JO','PPC.JO','TBS.JO']
START_DATE = '2020-01-01'
END_DATE = pd.to_datetime('today').strftime('%Y-%m-%d')
N_CLUSTERS = 10 # You can change the number of clusters (K)
TRADING_DAYS_PER_YEAR = 252
# Define fundamental features to retrieve
FUNDAMENTAL_FEATURES = ['priceToBook', 'trailingPE', 'debtToEquity', 'dividendYield']

# --- 1. Data Retrieval and Feature Engineering ---

print(f"--- Fetching Data for {STOCKS} ---")
# Download historical adjusted close prices
try:
    all_data = yf.download(STOCKS, start=START_DATE, end=END_DATE, auto_adjust=False, progress=False)
    data = all_data['Adj Close']
except Exception as e:
    print(f"An error occurred during data download: {e}")
    print("Please check if the tickers are correct and if yfinance is installed properly.")
    exit()

# Check if data was successfully loaded
if data.empty:
    print("Error: Downloaded data is empty. Check dates and tickers.")
    exit()

# Calculate Daily Log Returns
log_returns = np.log(data / data.shift(1)).dropna() # Keep this DataFrame for the new graphs

# Calculate Annualized Return and Volatility
features_df = pd.DataFrame(index=STOCKS)
features_df['Return'] = log_returns.mean() * TRADING_DAYS_PER_YEAR
features_df['Volatility'] = log_returns.std() * np.sqrt(TRADING_DAYS_PER_YEAR)


# ==============================================================================
## ‚ú® Comprehensive Fundamental Data Retrieval
# ==============================================================================

print("\n--- Fetching Comprehensive Fundamental Data ---")
fundamental_data = {feat: {} for feat in FUNDAMENTAL_FEATURES}

for ticker in STOCKS:
    try:
        info = yf.Ticker(ticker).info
        for feature in FUNDAMENTAL_FEATURES:
            # Check if the feature exists and handle non-positive P/E ratios
            value = info.get(feature)
            if value is not None and feature == 'trailingPE' and value <= 0:
                 fundamental_data[feature][ticker] = np.nan
            else:
                 fundamental_data[feature][ticker] = value
    except Exception:
        # Set to NaN if data retrieval fails for any stock
        for feature in FUNDAMENTAL_FEATURES:
            fundamental_data[feature][ticker] = np.nan

# Merge fundamental data into the features DataFrame
for feature, data_dict in fundamental_data.items():
    features_df[feature.replace('trailingPE', 'PE_Ratio').replace('priceToBook', 'Price_to_Book').replace('debtToEquity', 'Debt_to_Equity').replace('dividendYield', 'Dividend_Yield')] = pd.Series(data_dict)


# Clean and Prepare Data for Clustering
# Drop any stock that is missing data (e.g., failed retrieval of required features)
features_df.dropna(inplace=True)
print("\nFinal Features for Clustering:")
print(features_df)
print(f"\nTotal stocks used for clustering: {len(features_df)} out of {len(STOCKS)} initial tickers.")

# Separate the features for the two clustering scenarios
X_2D = features_df[['Volatility', 'Return']].values
X_3D = features_df[['Volatility', 'Return', 'Price_to_Book']].values
tickers = features_df.index.tolist()

# --- 2. K-Means Clustering (2D - Volatility & Return) ---

if len(features_df) >= N_CLUSTERS:
    print(f"\n--- Performing 2D K-Means Clustering (K={N_CLUSTERS}) ---")

    # 2a. Standardize the features (Crucial for K-Means)
    scaler_2d = StandardScaler()
    X_scaled_2D = scaler_2d.fit_transform(X_2D)

    # 2b. Apply K-Means
    kmeans_2d = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)
    features_df['Cluster_2D'] = kmeans_2d.fit_predict(X_scaled_2D)

    # --- 3. Plotting the 2D Results (Volatility vs. Return) ---
    plt.figure(figsize=(10, 6))

    scatter_2d = plt.scatter(
        features_df['Volatility'],
        features_df['Return'],
        c=features_df['Cluster_2D'],
        cmap='viridis',
        s=100,
        alpha=0.8,
        edgecolors='w'
    )

    for i, txt in enumerate(tickers):
        plt.annotate(
            txt,
            (features_df['Volatility'][i] + 0.005, features_df['Return'][i] + 0.0005),
            fontsize=9
        )

    plt.title(f'Stock Clustering: Volatility vs. Annualized Return (K={N_CLUSTERS})')
    plt.xlabel('Annualized Volatility (Risk)')
    plt.ylabel('Annualized Return')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.colorbar(scatter_2d, label='Cluster ID')
    plt.show()
    #
else:
    print(f"\nSkipping 2D clustering: Not enough unique data points ({len(features_df)}) for K={N_CLUSTERS}.")

# -----------------------------------------------------------------------------

# --- 4. K-Means Clustering (3D - Volatility, Return, Price to Book) ---

if len(features_df) >= N_CLUSTERS:
    print(f"\n--- Performing 3D K-Means Clustering (K={N_CLUSTERS}) ---")

    # 4a. Standardize the 3D features
    scaler_3d = StandardScaler()
    X_scaled_3D = scaler_3d.fit_transform(X_3D)

    # 4b. Apply K-Means
    kmeans_3d = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)
    features_df['Cluster_3D'] = kmeans_3d.fit_predict(X_scaled_3D)

    # --- 5. Plotting the 3D Results ---
    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection='3d')

    scatter_3d = ax.scatter(
        features_df['Volatility'],
        features_df['Return'],
        features_df['Price_to_Book'],
        c=features_df['Cluster_3D'],
        cmap='viridis',
        s=100,
        alpha=0.8,
        edgecolors='w'
    )

    for i, txt in enumerate(tickers):
        ax.text(
            features_df['Volatility'][i],
            features_df['Return'][i],
            features_df['Price_to_Book'][i],
            txt,
            fontsize=8
        )

    ax.set_title(f'Stock Clustering: Volatility, Return, and Price-to-Book (K={N_CLUSTERS})')
    ax.set_xlabel('Annualized Volatility')
    ax.set_ylabel('Annualized Return')
    ax.set_zlabel('Price to Book Ratio')

    fig.colorbar(scatter_3d, label='Cluster ID', shrink=0.7, aspect=10)
    plt.show()
    #
else:
    print(f"\nSkipping 3D clustering: Not enough unique data points ({len(features_df)}) for K={N_CLUSTERS}.")

# -----------------------------------------------------------------------------

# ==============================================================================
## üìä New Section: Daily Returns Distribution Graphs
# ==============================================================================

print("\n" + "="*80)
print("üìà DAILY RETURNS DISTRIBUTION (NORMALITY CHECK)")
print("="*80)

# Set up the figure grid dynamically based on the number of available stocks
valid_tickers = log_returns.columns.tolist()
num_plots = len(valid_tickers)
cols = 4 # 4 plots per row
rows = int(np.ceil(num_plots / cols))

fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))
axes = axes.flatten() # Flatten array for easy indexing

# Generate a plot for each stock's daily log returns
for i, ticker in enumerate(valid_tickers):
    ax = axes[i]

    # Plot the Kernel Density Estimate (KDE) of the daily returns
    log_returns[ticker].plot(kind='kde', ax=ax, title=f'{ticker} Daily Return Distribution', color='darkblue')

    # Calculate Mean and Std Dev for the normal distribution overlay
    mu = log_returns[ticker].mean()
    sigma = log_returns[ticker].std()

    # Create the theoretical normal distribution curve
    x_range = np.linspace(log_returns[ticker].min(), log_returns[ticker].max(), 100)
    normal_curve = (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x_range - mu) / sigma)**2)

    # Overlay the theoretical normal distribution (optional, but instructive)
    ax.plot(x_range, normal_curve, color='red', linestyle='--', label='Normal Fit')

    # Add mean line (center)
    ax.axvline(mu, color='green', linestyle=':', linewidth=1, label=f'Mean: {mu:.4f}')

    ax.set_xlabel('Daily Log Return')
    ax.set_ylabel('Density')
    ax.legend(loc='upper left', fontsize=7)
    ax.grid(axis='y', linestyle=':', alpha=0.6)

# Hide any unused subplots
for j in range(num_plots, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()
#


# -----------------------------------------------------------------------------

# ==============================================================================
## üìà Fundamental Analysis and Investment Recommendation Report
# ==============================================================================

print("\n" + "="*80)
print("üìä FUNDAMENTAL ANALYSIS AND INVESTMENT RECOMMENDATION REPORT")
print("="*80)

# Calculate Median Metrics for comparison
median_pe = features_df['PE_Ratio'].median()
median_pb = features_df['Price_to_Book'].median()
median_de = features_df['Debt_to_Equity'].median()
median_vol = features_df['Volatility'].median()
median_ret = features_df['Return'].median()

print(f"Market-Wide Median P/E Ratio: {median_pe:,.2f}")
print(f"Market-Wide Median P/B Ratio: {median_pb:,.2f}")
print(f"Market-Wide Median Debt-to-Equity: {median_de:,.2f}")
print("---")

recommendations = []
report_data = []

for index, row in features_df.iterrows():
    ticker = index

    # --- Valuation Assessment ---
    # Undervalued: Below median P/E and P/B
    # Overvalued: Above median P/E and P/B
    if row['PE_Ratio'] < median_pe and row['Price_to_Book'] < median_pb:
        valuation = "**Undervalued** (Low P/E & P/B)"
    elif row['PE_Ratio'] > median_pe and row['Price_to_Book'] > median_pb:
        valuation = "**Overvalued** (High P/E & P/B)"
    else:
        valuation = "Fairly Valued/Mixed"

    # --- Risk Assessment ---
    # Low Risk: Below median Volatility and low Debt-to-Equity
    if row['Volatility'] < median_vol and row['Debt_to_Equity'] < median_de:
        risk = "Low Risk / Low Leverage"
    elif row['Volatility'] > median_vol and row['Debt_to_Equity'] > median_de:
        risk = "**High Risk** / High Leverage"
    else:
        risk = "Moderate Risk"

    # --- Investment Suitability ---
    suitability = "Neutral"
    if 'Undervalued' in valuation and row['Return'] > median_ret:
        suitability = "**Strong BUY** (Undervalued with High Returns)"
    elif 'Undervalued' in valuation:
        suitability = "**Potential BUY** (Undervalued Value Play)"
    elif 'Overvalued' in valuation and row['Volatility'] > median_vol:
        suitability = "**Avoid/Sell** (Overvalued and High Risk)"

    recommendations.append(suitability)

    report_data.append({
        'Ticker': ticker,
        'Valuation': valuation,
        'Risk/Leverage': risk,
        'Suitability': suitability,
        'PE_Ratio': row['PE_Ratio'], # Keep numeric for plotting
        'P/B_Ratio': row['Price_to_Book'], # Keep numeric for plotting
        'Div_Yield': f"{row['Dividend_Yield'] * 100:,.2f}%",
        'Debt/Equity': row['Debt_to_Equity'], # Keep numeric for plotting
        'Volatility_Numeric': row['Volatility'], # Keep numeric for plotting
        'Return_Numeric': row['Return'], # Keep numeric for plotting
    })

# Create the final report DataFrame
report_df = pd.DataFrame(report_data).set_index('Ticker')

print("\n### Fundamental Metrics Snapshot")
print(report_df[['PE_Ratio', 'P/B_Ratio', 'Debt/Equity', 'Div_Yield']].applymap(lambda x: f"{x:,.2f}" if isinstance(x, (int, float)) else x))

print("\n### Valuation & Investment Recommendation Summary")
# Print the final report table, prioritizing the recommendation and valuation
print(report_df[['Valuation', 'Risk/Leverage', 'Suitability']])

# Final conclusive statement
strong_buys = report_df[report_df['Suitability'].str.contains('Strong BUY')].index.tolist()
undervalued = report_df[report_df['Valuation'].str.contains('Undervalued')].index.tolist()
overvalued = report_df[report_df['Valuation'].str.contains('Overvalued')].index.tolist()

print("\n---")
print(f"üî• **Top Undervalued Stocks (Value and Growth):** {', '.join(strong_buys) if strong_buys else 'None found based on criteria.'}")
print(f"üí∞ **General Undervalued Stocks (Value Play):** {', '.join(undervalued) if undervalued else 'None found based on criteria.'}")
print(f"üõë **Most Overvalued/Risky Stocks to Avoid:** {', '.join(overvalued) if overvalued else 'None found based on criteria.'}")
print("---")


# ==============================================================================
## üìä Graphical Valuation Analysis
# ==============================================================================

print("\n" + "="*80)
print("GRAPHICAL VALUATION ANALYSIS")
print("="*80)

# --- Graph 1: P/E Ratio vs. Annualized Return ---
plt.figure(figsize=(10, 6))
plt.scatter(report_df['PE_Ratio'], report_df['Return_Numeric'], c='teal', s=100, alpha=0.7, edgecolors='w')

# Median Lines
plt.axvline(median_pe, color='r', linestyle='--', label=f'Median P/E ({median_pe:,.2f})')
plt.axhline(median_ret, color='g', linestyle='--', label=f'Median Return ({median_ret:,.2f})')

# Annotate points
for i, txt in enumerate(report_df.index):
    plt.annotate(txt, (report_df['PE_Ratio'].iloc[i] * 1.05, report_df['Return_Numeric'].iloc[i] * 1.05))

plt.title('Valuation: P/E Ratio vs. Annualized Return')
plt.xlabel('Price-to-Earnings Ratio (P/E)')
plt.ylabel('Annualized Return')
plt.grid(True, linestyle=':', alpha=0.5)
plt.legend()
plt.show()
#


# --- Graph 2: P/B Ratio vs. Annualized Return ---
plt.figure(figsize=(10, 6))
plt.scatter(report_df['P/B_Ratio'], report_df['Return_Numeric'], c='purple', s=100, alpha=0.7, edgecolors='w')

# Median Lines
plt.axvline(median_pb, color='r', linestyle='--', label=f'Median P/B ({median_pb:,.2f})')
plt.axhline(median_ret, color='g', linestyle='--', label=f'Median Return ({median_ret:,.2f})')

# Annotate points
for i, txt in enumerate(report_df.index):
    plt.annotate(txt, (report_df['P/B_Ratio'].iloc[i] * 1.05, report_df['Return_Numeric'].iloc[i] * 1.05))

plt.title('Valuation: P/B Ratio vs. Annualized Return')
plt.xlabel('Price-to-Book Ratio (P/B)')
plt.ylabel('Annualized Return')
plt.grid(True, linestyle=':', alpha=0.5)
plt.legend()
plt.show()
#


# --- Graph 3: Debt-to-Equity vs. Volatility (Risk Assessment) ---
plt.figure(figsize=(10, 6))
plt.scatter(report_df['Debt/Equity'], report_df['Volatility_Numeric'], c='red', s=100, alpha=0.7, edgecolors='w')

# Median Lines
plt.axvline(median_de, color='b', linestyle='--', label=f'Median Debt/Equity ({median_de:,.2f})')
plt.axhline(median_vol, color='orange', linestyle='--', label=f'Median Volatility ({median_vol:,.2f})')

# Annotate points
for i, txt in enumerate(report_df.index):
    plt.annotate(txt, (report_df['Debt/Equity'].iloc[i] * 1.05, report_df['Volatility_Numeric'].iloc[i] * 1.05))

plt.title('Risk Assessment: Debt-to-Equity vs. Annualized Volatility')
plt.xlabel('Debt-to-Equity Ratio (Leverage)')
plt.ylabel('Annualized Volatility (Risk)')
plt.grid(True, linestyle=':', alpha=0.5)
plt.legend()
plt.show()
#